[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/blog-6/blog-6.html",
    "href": "posts/blog-6/blog-6.html",
    "title": "Blog 6- Perceptron",
    "section": "",
    "text": "Perceptron Code"
  },
  {
    "objectID": "posts/blog-6/blog-6.html#abstract",
    "href": "posts/blog-6/blog-6.html#abstract",
    "title": "Blog 6- Perceptron",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I build the perceptron algorithim, which creates and optimizes a linear descsion boundary for binarily classifiable data. I find that the perceptron algorithim works on data over 2 diminesions, but it doesn’t work on data that is non-linearly seperarable."
  },
  {
    "objectID": "posts/blog-6/blog-6.html#about-perceptron.grad",
    "href": "posts/blog-6/blog-6.html#about-perceptron.grad",
    "title": "Blog 6- Perceptron",
    "section": "About perceptron.grad()",
    "text": "About perceptron.grad()\nThe grad() method computes the perceptron update by first converting labels to ({-1,1}) and calculating the scores ( s = Xw ). It then identifies misclassified points using ( s_i y_i &lt; 0 ) and creates a mask to apply updates only to those points. The gradient is computed as the mean of ( y x ) over the misclassified points, ensuring mini-batch learning. Finally, the update is scaled by a learning rate ( ), correctly implementing the perceptron update rule."
  },
  {
    "objectID": "posts/blog-6/blog-6.html#part-a-making-perceptron",
    "href": "posts/blog-6/blog-6.html#part-a-making-perceptron",
    "title": "Blog 6- Perceptron",
    "section": "Part A: Making Perceptron",
    "text": "Part A: Making Perceptron\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\nimport torch\ntorch.manual_seed(1234)\n\nimport torch\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\n\ntorch.manual_seed(123)\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nbatch_size = 5 \nmax_iter= 0\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data mini-batch    \n    # ix = torch.randperm(X.size(0))[:batch_size]  \n    # X_batch = X[ix, :]\n    # y_batch = y[ix]\n    # local_loss= p.loss(X_batch, y_batch).item()\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    #opt.step(X_batch, y_batch)\n    max_iter+=1\n\n\nprint(loss)\nprint(p.w)\nw = p.w\n\ntensor(0.)\ntensor([ 0.4089,  0.3166, -0.3581])"
  },
  {
    "objectID": "posts/blog-6/blog-6.html#part-b-expirements",
    "href": "posts/blog-6/blog-6.html#part-b-expirements",
    "title": "Blog 6- Perceptron",
    "section": "Part B: Expirements",
    "text": "Part B: Expirements\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w, -1, 2, ax, color = \"black\")\nprint(max_iter)\n\n96\n\n\n\n\n\n\n\n\n\nHere I create linerarly seperable data so we can see what happens to the loss, descsion boundary and weights through iterations of the training loop.\n\ntorch.manual_seed(1234)\nimport matplotlib.pyplot as plt\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    if current_ax &gt;= 6:\n        break\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    \n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        i_int = i.item() \n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = marker_map.get(2 * y[i].item() - 1, 0) )\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n/var/folders/gx/d6zl9l4908q7p_vk4j3x06x00000gn/T/ipykernel_35223/4230432632.py:46: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker (0).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = marker_map.get(2 * y[i].item() - 1, 0) )\n/var/folders/gx/d6zl9l4908q7p_vk4j3x06x00000gn/T/ipykernel_35223/4230432632.py:46: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker (1).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = marker_map.get(2 * y[i].item() - 1, 0) )\n\n\n\n\n\n\n\n\n\nHere we see the evolution of the loss function with linearly seperable data. This visualization shows the evolution of the loss for perceptron every iteration. We see improvement in the drawing of the lines as perceptron continutes to update the weights to shift the line, eventually achieving a very low loss of 0. This is becuase perceptron identifies the misclassified points and re-fits until there are no misclassified points.\n\n#creating non-linearly seperable data\ntorch.manual_seed(1234)\n\ndef nonlinearly_separable_data(n_points=300, noise=0.2, p_dims=2):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    \n    # Increase noise to create significant overlap\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    \n    # Add bias term (column of ones)\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n\n    return X, y\n\nX, y = nonlinearly_separable_data(n_points=300, noise=1.0)\n\n\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter= 0\n\nwhile loss &gt; 0 and max_iter &lt; 1001: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iter += 1\n\nw= p.w\nscores = p.score(X)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nThe data I generate here has a lot of noise so it is non-linearly seperarable. This is so we can see what happens to the descison boundary, score and weights as it runs through iterations. I added a breakout point (1000 iterations) in the training loop\n\ntorch.manual_seed(12345)\nimport matplotlib.pyplot as plt\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nw= p.w\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    if current_ax &gt;= 6:\n        break\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = marker_map.get(2 * y[i].item() - 1, 0) )\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        loss = p.loss(X, y).item()\n        scores = p.score(X).mean().item()\n        ax.set_title(f\"loss = {loss:.3f}, Score = {scores:.3f}\", fontsize= 7)\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n/var/folders/gx/d6zl9l4908q7p_vk4j3x06x00000gn/T/ipykernel_35223/2906442253.py:46: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker (0).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = marker_map.get(2 * y[i].item() - 1, 0) )\n/var/folders/gx/d6zl9l4908q7p_vk4j3x06x00000gn/T/ipykernel_35223/2906442253.py:46: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker (1).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = marker_map.get(2 * y[i].item() - 1, 0) )\n\n\n\n\n\n\n\n\n\nWith this non-linearly seperable data, we see that the scores and the loss keeps changing, with no quantifiable improvements in the score. Instead, since the data can’t be seperated, it appears to be a random generation of descision boundaries, where the score changes drastically every time. If we didn’t cap it by setting a max_iter, it would run forever because the loss will never be 0.\n\nimport torch\ntorch.manual_seed(1234)\n\nimport torch\n\ntorch.manual_seed(1234)\n\ndef perceptron_data_5d(n_points = 300, noise = 0.2, p_dims = 5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter= 0\n\nwhile loss &gt; 0 and max_iter &lt; 1001: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iter += 1\n\nw= p.w\nscores = p.score(X)\n\nHere I run the training loop on the above data, which has 5 dimensions. I store the scores for each iteration in scores and then plot that over the number of iterations to look at how the score changes over time.\n\nplt.plot(scores, label=\"Score\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Score\")\nplt.title(\"Evolution of Score over Training\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe see that the score increases over the training period for our 5d data, with the improvement happening at iteration 150 (halfway through the training loop). This proves that perceptron works for data with higher dimensions than just 2d, as we see score improvements for 5d data."
  },
  {
    "objectID": "posts/blog-6/blog-6.html#minibatch",
    "href": "posts/blog-6/blog-6.html#minibatch",
    "title": "Blog 6- Perceptron",
    "section": "Minibatch",
    "text": "Minibatch\n\nimport torch\ntorch.manual_seed(123)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\n# k= 1\ntorch.manual_seed(123)\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nk= 1\nmax_iter= 0\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data mini-batch    \n    ix = torch.randperm(X.size(0))[:k]  \n    X_batch = X[ix, :]\n    y_batch = y[ix]\n    local_loss= p.loss(X_batch, y_batch).item()\n    # i = torch.randint(n, size = (1,))\n    # x_i = X[[i],:]\n    # y_i = y[i]\n    # local_loss = p.loss(x_i, y_i).item()\n\n\n    # perform a perceptron update using the random data point\n    # opt.step(x_i, y_i)\n    opt.step(X_batch, y_batch)\n    max_iter+=1\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w, -1, 2, ax, color = \"black\")\nprint(max_iter)\n\n96\n\n\n\n\n\n\n\n\n\nWhen k=1, we see that minibatch performs similarly to regular perceptron. It takes the same number of iterations it did in the first example (96) to create a descsion boundary that sucessfully divides the data, showing similar results with a similar runtime.\n\n# batch size equals 10\ntorch.manual_seed(123)\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nk= 10\nmax_iter= 0\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data mini-batch    \n    ix = torch.randperm(X.size(0))[:k]  \n    X_batch = X[ix, :]\n    y_batch = y[ix]\n    local_loss= p.loss(X_batch, y_batch).item()\n    # i = torch.randint(n, size = (1,))\n    # x_i = X[[i],:]\n    # y_i = y[i]\n    # local_loss = p.loss(x_i, y_i).item()\n\n\n    # perform a perceptron update using the random data point\n    # opt.step(x_i, ay_i)\n    opt.step(X_batch, y_batch)\n    max_iter+=1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w, -1, 2, ax, color = \"black\")\nprint(max_iter)\n\n8\n\n\n\n\n\n\n\n\n\nWhen the batch size is equal to 10, we see a decrease in the number of iterations it takes (only 8 on the same data) for the loss to be equal to 0. This is because it is computing weight updates in bigger batches, which makes it more accurate.\n\n# batch size equals n\nX, y = nonlinearly_separable_data(n_points = 300, noise = 1)\n\ntorch.manual_seed(123)\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nk= n\nmax_iter= 0\nwhile loss &gt; .24: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data mini-batch    \n    ix = torch.randperm(X.size(0))[:k]  \n    X_batch = X[ix, :]\n    y_batch = y[ix]\n    local_loss= p.loss(X_batch, y_batch).item()\n    # i = torch.randint(n, size = (1,))\n    # x_i = X[[i],:]\n    # y_i = y[i]\n    # local_loss = p.loss(x_i, y_i).item()\n\n\n    # perform a perceptron update using the random data point\n    # opt.step(x_i, ay_i)\n    opt.step(X_batch, y_batch)\n    max_iter+=1\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w, -1, 2, ax, color = \"black\")\nprint(loss)\nprint(max_iter)\n\ntensor(0.2400)\n184\n\n\n\n\n\n\n\n\n\nHere we can see that when the batch size is equal to the number of datapoints n, loss converges even when the data is non-linearly seperable. In this case, the loss converged to .23."
  },
  {
    "objectID": "posts/blog-6/blog-6.html#runtime",
    "href": "posts/blog-6/blog-6.html#runtime",
    "title": "Blog 6- Perceptron",
    "section": "Runtime",
    "text": "Runtime\nThe perceptron algorithim involves checking each datapoint and updating the weights if the data points are misclassified. This means that it checks n datapoints, so O(n). However, for each datapoint, perceptron takes the dot product between the feature vector and the weight vector, so the overall time is O(n*p), therefore it depends on both the number of datapoints and the number of features.\nUsing mini-batch, it is instead O(mp) instead of O(np), because we are now running for m batches instead of n datapoints, where p is still the number of features."
  },
  {
    "objectID": "posts/blog-6/blog-6.html#conclusion",
    "href": "posts/blog-6/blog-6.html#conclusion",
    "title": "Blog 6- Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I write and run the perceptron algoritihim to demonstrate it’s effectivness at finding descsion boundaries on linearly seperable data. I also includ mini-batching, which updates the weights in batches instead of one point at a time. I show expierements to show how perceptron works with different types of data in different dimensions and discuss the runtime."
  },
  {
    "objectID": "posts/blog-1/blog_1.html",
    "href": "posts/blog-1/blog_1.html",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "image.png\n\n\n\n\nIn my blog post, I explore the best features to identify penguin species from the Palmer Penguins data set created by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Dr. Kristen Gorman ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Palmer Station. I first explore the significance of some variables through data visualization, trying to glean key differences in penguin species, one of which appears to be island. Using a brute force search of all the possible combinations of one qualitative and two quantitative variables, I find that island, culmen length, and culmen depth are the three best features to predict the test set with 100% accuracy.\n\n\n\n\nimport pandas as pd\n\n# reading in the training data\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url) \n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can see from the head of the training data some of the key variables to look at: island, clutch completion, culmen length/depth, flipper length and body mass. These variables jump out as the oens where we will likely see the biggest differences between species.\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #encoding species\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #dropping uncessary variables\n  df = df[df[\"Sex\"] != \".\"] \n  df = df.dropna() #cleaning data\n  y = le.transform(df[\"Species\"]) #encoding species\n  df = df.drop([\"Species\"], axis = 1) \n  df = pd.get_dummies(df) \n  return df, y\n\n#creating test and train datasets\nX_train, y_train = prepare_data(train) \n\nIn the data preperation section, I prepare the data by dropping the variables that aren’t helpful for the analysis. I also use the label encoder function to encode the target variable (species). I also clean the data so I can prepare the features and the labels for training.\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n## graph1 scatter + weight distribtuion\n\ndata= train\ngraph1 = sns.jointplot(data= data, x= \"Flipper Length (mm)\", y= \"Body Mass (g)\", hue = \"Species\")\nplt.show\n\n\n\n\n\n\n\n\nThe main takeaway I had from this graph is that flipper length is a great identifier for the Gentoo penguins, as they all have flipper lengths at about 208 mm or above, which is much longer than the average of an Adelie or a Chinstrap penguin. However, the body mass and flipper length are similar for and Chinstrap penguins, which means that they might not be the best identifier of a penguin. We can see that the distributions for body mass of the two species have almost complete overlap, with Adelie penguins having less devation in mass by penguin.\n\n# graph 2: catplot\ndata = train\ngraph2= sns.catplot(data= data, kind= \"swarm\", x= \"Island\", y=\"Culmen Length (mm)\", hue= \"Species\")\n\n\n\n\n\n\n\n\nThis visualization is very helpful for deciding which categories to model. The first thing I notice is that the Chinstrap and Gentoo species are native to one island and one island only (dream and biscoe, respectiveley). While the Adelie penguins appear on all 3 islands, they are the only ones that appear on Torgenrsen, so any penguin found on Torgersen should automatically be identified as a Adelie. Culmen length is also a good diffentiator,\n\ndata.groupby(\"Species\").aggregate({\"Flipper Length (mm)\" : \"mean\", \n                                   \"Culmen Length (mm)\" : \"mean\",\n                                    \"Culmen Depth (mm)\" : \"mean\",\n                                    \"Body Mass (g)\" : [\"mean\", \"min\", \"max\"]})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nmean\nmean\nmean\nmin\nmax\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n3718.487395\n2850.0\n4725.0\n\n\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n3743.421053\n2700.0\n4800.0\n\n\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n5039.948454\n3950.0\n6300.0\n\n\n\n\n\n\n\nThis table shows clear distinctions between the penguins for the different quantitative variables highlighted here. For example, while Adelie and Chinstraps have similar flipper lengths, masses, and culmen depths, they vary vastly in clumen length. Gentoos can be differentiated through their much larger average body mass and longer flippers. There shouild be very little overlap betrween Adelie and Gentoo, as they very significantly on the different quantitative variables.\n\n\n\n\n%%capture\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Stage\", \"Sex\"] \nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\" ]\n\nbest_score= 0\nfor qual in all_qual_cols: #iterating through all possible combinations of columns\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2): \n    cols = list(pair) + qual_cols\n    #print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train) #fitting the model\n    score = LR.score(X_train[cols], y_train) #scoring the model\n    if score &gt; best_score: #saving the best score\n      best_score = score\n      best_cols = cols\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\nThe way I approach selecting the best features is to use a nested loop to run through all possible combinations of one qualitative and two quantitative variables. I keep track of the training model with the best score and save those features.\n\n%%capture\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train) #running the model on the best features\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nHere I run the training model on my best selected features and load in the test data.\n\n#preparing the test set and testing it\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nI calculate the score here for the model established on the testing data and get an accuracy score of 100%.\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nprint(best_cols)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# plotting the decision reigon plot\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(LR, X_train[best_cols], y_train)\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the decision plots for the training and the testing data sets. As we saw in the explore section, certain species are native to certain islands. In the trainig data, we see that some Chinstrap penguins are on the boundary between Chinstrap and Gentoo reigons (causing slight misclassification), but due to the subset of data in the testing set, none of these penguins are super close to the boarder, so everything is classified correctly.\n\nfrom sklearn.metrics import confusion_matrix\n\n# creating confusion matricies for the test and train datasets.\ny_test_pred = LR.predict(X_test[best_cols])\nConfusion_test = confusion_matrix(y_test, y_test_pred)\nprint(Confusion_test) \ny_train_pred = LR.predict(X_train[best_cols])\nConfusion_train = confusion_matrix(y_train, y_train_pred)\nprint(Confusion_train)\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n[[108   0   0]\n [  1  55   0]\n [  0   0  92]]\n\n\nSince there are no errors in the test set, the first confusion matrix is diagonalizable. There are no penguins that are classified incorrectly. Because of this, I also created the same matrix for the train dataset, where one Chinstrap penguin was classified an Adelie. I wonder if this could be in part because the number of penguins in each set where drastically different, and since there a lot more Adelie I wonder if penguins were more likely to be classified as such.\n\n\n\nMy key takeaway from the analysis is that when using one feature on it’s own, or a pair of correlating features only (i.e. culmen length and culmen depth), logistic regression is decently accurate. The more features you pair together, the more accurate the logistic regression becomes, although the computing power escalates quickly. Island is a great quantitative identifier for penguins, as shown in figure 2 because Chinstraps and Gentoos exist only on different islands, so there is no overlap or mispredicting.\nOn competing my first blog post, I found the process very rewarding. I like being able to write out my ideas and complete my analysis all in the same document. I haven’t coded in a while, so debugging at first was difficult, but it got easier as I went on. I learned that, with both the coding and the discussion, it’s often best to step away for a bit and come back to the work with fresh eyes. That helped me when I was looking for significance in the explore step, and throughout debugging my model and the ensuing discussion."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#abstract",
    "href": "posts/blog-1/blog_1.html#abstract",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "In my blog post, I explore the best features to identify penguin species from the Palmer Penguins data set created by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Dr. Kristen Gorman ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Palmer Station. I first explore the significance of some variables through data visualization, trying to glean key differences in penguin species, one of which appears to be island. Using a brute force search of all the possible combinations of one qualitative and two quantitative variables, I find that island, culmen length, and culmen depth are the three best features to predict the test set with 100% accuracy."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#setup",
    "href": "posts/blog-1/blog_1.html#setup",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "import pandas as pd\n\n# reading in the training data\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url) \n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can see from the head of the training data some of the key variables to look at: island, clutch completion, culmen length/depth, flipper length and body mass. These variables jump out as the oens where we will likely see the biggest differences between species."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#data-preperation",
    "href": "posts/blog-1/blog_1.html#data-preperation",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #encoding species\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #dropping uncessary variables\n  df = df[df[\"Sex\"] != \".\"] \n  df = df.dropna() #cleaning data\n  y = le.transform(df[\"Species\"]) #encoding species\n  df = df.drop([\"Species\"], axis = 1) \n  df = pd.get_dummies(df) \n  return df, y\n\n#creating test and train datasets\nX_train, y_train = prepare_data(train) \n\nIn the data preperation section, I prepare the data by dropping the variables that aren’t helpful for the analysis. I also use the label encoder function to encode the target variable (species). I also clean the data so I can prepare the features and the labels for training."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#explore",
    "href": "posts/blog-1/blog_1.html#explore",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n## graph1 scatter + weight distribtuion\n\ndata= train\ngraph1 = sns.jointplot(data= data, x= \"Flipper Length (mm)\", y= \"Body Mass (g)\", hue = \"Species\")\nplt.show\n\n\n\n\n\n\n\n\nThe main takeaway I had from this graph is that flipper length is a great identifier for the Gentoo penguins, as they all have flipper lengths at about 208 mm or above, which is much longer than the average of an Adelie or a Chinstrap penguin. However, the body mass and flipper length are similar for and Chinstrap penguins, which means that they might not be the best identifier of a penguin. We can see that the distributions for body mass of the two species have almost complete overlap, with Adelie penguins having less devation in mass by penguin.\n\n# graph 2: catplot\ndata = train\ngraph2= sns.catplot(data= data, kind= \"swarm\", x= \"Island\", y=\"Culmen Length (mm)\", hue= \"Species\")\n\n\n\n\n\n\n\n\nThis visualization is very helpful for deciding which categories to model. The first thing I notice is that the Chinstrap and Gentoo species are native to one island and one island only (dream and biscoe, respectiveley). While the Adelie penguins appear on all 3 islands, they are the only ones that appear on Torgenrsen, so any penguin found on Torgersen should automatically be identified as a Adelie. Culmen length is also a good diffentiator,\n\ndata.groupby(\"Species\").aggregate({\"Flipper Length (mm)\" : \"mean\", \n                                   \"Culmen Length (mm)\" : \"mean\",\n                                    \"Culmen Depth (mm)\" : \"mean\",\n                                    \"Body Mass (g)\" : [\"mean\", \"min\", \"max\"]})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nmean\nmean\nmean\nmin\nmax\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n3718.487395\n2850.0\n4725.0\n\n\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n3743.421053\n2700.0\n4800.0\n\n\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n5039.948454\n3950.0\n6300.0\n\n\n\n\n\n\n\nThis table shows clear distinctions between the penguins for the different quantitative variables highlighted here. For example, while Adelie and Chinstraps have similar flipper lengths, masses, and culmen depths, they vary vastly in clumen length. Gentoos can be differentiated through their much larger average body mass and longer flippers. There shouild be very little overlap betrween Adelie and Gentoo, as they very significantly on the different quantitative variables."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#model",
    "href": "posts/blog-1/blog_1.html#model",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "%%capture\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Stage\", \"Sex\"] \nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\" ]\n\nbest_score= 0\nfor qual in all_qual_cols: #iterating through all possible combinations of columns\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2): \n    cols = list(pair) + qual_cols\n    #print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train) #fitting the model\n    score = LR.score(X_train[cols], y_train) #scoring the model\n    if score &gt; best_score: #saving the best score\n      best_score = score\n      best_cols = cols\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\nThe way I approach selecting the best features is to use a nested loop to run through all possible combinations of one qualitative and two quantitative variables. I keep track of the training model with the best score and save those features.\n\n%%capture\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train) #running the model on the best features\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nHere I run the training model on my best selected features and load in the test data.\n\n#preparing the test set and testing it\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nI calculate the score here for the model established on the testing data and get an accuracy score of 100%."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#evaluate",
    "href": "posts/blog-1/blog_1.html#evaluate",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nprint(best_cols)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# plotting the decision reigon plot\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(LR, X_train[best_cols], y_train)\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the decision plots for the training and the testing data sets. As we saw in the explore section, certain species are native to certain islands. In the trainig data, we see that some Chinstrap penguins are on the boundary between Chinstrap and Gentoo reigons (causing slight misclassification), but due to the subset of data in the testing set, none of these penguins are super close to the boarder, so everything is classified correctly.\n\nfrom sklearn.metrics import confusion_matrix\n\n# creating confusion matricies for the test and train datasets.\ny_test_pred = LR.predict(X_test[best_cols])\nConfusion_test = confusion_matrix(y_test, y_test_pred)\nprint(Confusion_test) \ny_train_pred = LR.predict(X_train[best_cols])\nConfusion_train = confusion_matrix(y_train, y_train_pred)\nprint(Confusion_train)\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n[[108   0   0]\n [  1  55   0]\n [  0   0  92]]\n\n\nSince there are no errors in the test set, the first confusion matrix is diagonalizable. There are no penguins that are classified incorrectly. Because of this, I also created the same matrix for the train dataset, where one Chinstrap penguin was classified an Adelie. I wonder if this could be in part because the number of penguins in each set where drastically different, and since there a lot more Adelie I wonder if penguins were more likely to be classified as such."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#discussion",
    "href": "posts/blog-1/blog_1.html#discussion",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "My key takeaway from the analysis is that when using one feature on it’s own, or a pair of correlating features only (i.e. culmen length and culmen depth), logistic regression is decently accurate. The more features you pair together, the more accurate the logistic regression becomes, although the computing power escalates quickly. Island is a great quantitative identifier for penguins, as shown in figure 2 because Chinstraps and Gentoos exist only on different islands, so there is no overlap or mispredicting.\nOn competing my first blog post, I found the process very rewarding. I like being able to write out my ideas and complete my analysis all in the same document. I haven’t coded in a while, so debugging at first was difficult, but it got easier as I went on. I learned that, with both the coding and the discussion, it’s often best to step away for a bit and come back to the work with fresh eyes. That helped me when I was looking for significance in the explore step, and throughout debugging my model and the ensuing discussion."
  },
  {
    "objectID": "posts/blog-3/blog_3.html",
    "href": "posts/blog-3/blog_3.html",
    "title": "Blog 3- Auditing Bias",
    "section": "",
    "text": "In this blog post, I look at racial employment bias in Ohio through ACS data. I do this by creating a model using an optimized descision tree. From this model, I do an audit to detect racism, although race was not an attribute the data was trained on. I find that there is some racisim in the model, as their is lack of error rate balance for Black people, bi-racial people, Asian people and Indegenous people. I then analyze my findings in the broader context of employers using this algorithim in the hiring process, concluding that the use of this algorithim may prepetuate systemic racism, and it is to not be used unless in conjunction with diverse hiring practices."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#abstract",
    "href": "posts/blog-3/blog_3.html#abstract",
    "title": "Blog 3- Auditing Bias",
    "section": "",
    "text": "In this blog post, I look at racial employment bias in Ohio through ACS data. I do this by creating a model using an optimized descision tree. From this model, I do an audit to detect racism, although race was not an attribute the data was trained on. I find that there is some racisim in the model, as their is lack of error rate balance for Black people, bi-racial people, Asian people and Indegenous people. I then analyze my findings in the broader context of employers using this algorithim in the hiring process, concluding that the use of this algorithim may prepetuate systemic racism, and it is to not be used unless in conjunction with diverse hiring practices."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#example",
    "href": "posts/blog-3/blog_3.html#example",
    "title": "Blog 3- Auditing Bias",
    "section": "Example",
    "text": "Example\n\nJust what the begining of the lab walks through, grader can skip\n\n\nSetup for Example\n\n#loading and setting up data\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns\n\n\n\n\n#selecting recomended possible features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n60\n15.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n1\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n18.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n20\n18.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n33\n18.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n\n\n\n\n\n\n\nExample\n\n# Setting up the employment problem\nimport warnings\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data) \n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(99419, 15)\n(99419,)\n(99419,)\n\n\n\n#create train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n#creating model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression()) #scales and performs a LR for the model\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\ny_hat = model.predict(X_test)\nprint((y_hat == y_test).mean())\nprint((y_hat == y_test)[group_test == 1].mean())\nprint((y_hat == y_test)[group_test == 2].mean())\n\n0.7863608931804466\n0.7875706214689265\n0.7777164920022063"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#setup",
    "href": "posts/blog-3/blog_3.html#setup",
    "title": "Blog 3- Auditing Bias",
    "section": "Setup",
    "text": "Setup\n\n#loading and setting up data\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"OH\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000037\n3\n1\n908\n2\n39\n1013097\n12\n82\n...\n10\n12\n0\n11\n20\n22\n22\n12\n10\n20\n\n\n1\nP\n2018GQ0000068\n3\n1\n4200\n2\n39\n1013097\n75\n45\n...\n76\n17\n77\n77\n78\n78\n15\n80\n77\n78\n\n\n2\nP\n2018GQ0000126\n3\n1\n3500\n2\n39\n1013097\n34\n50\n...\n33\n34\n3\n36\n35\n36\n34\n4\n35\n34\n\n\n3\nP\n2018GQ0000156\n3\n1\n400\n2\n39\n1013097\n53\n19\n...\n53\n53\n94\n55\n100\n96\n9\n7\n53\n50\n\n\n4\nP\n2018GQ0000168\n3\n1\n4700\n2\n39\n1013097\n14\n18\n...\n27\n14\n27\n14\n15\n27\n15\n26\n2\n15\n\n\n\n\n5 rows × 286 columns\n\n\n\nThe data is Census data from Ohio in 2018. Every observation is a person in the survey, and there are 119,000 people surveyed. From this survey, we glean demographic information and characteristics like employment, religon and military status.\n\n#selecting recomended possible features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n82\n12.0\n2\n16\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n2\n2\n6.0\n\n\n1\n45\n18.0\n5\n16\n1\nNaN\n2\n3.0\n4.0\n1\n1\n1\n2\n2.0\n1\n1\n6.0\n\n\n2\n50\n14.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n3\n19\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n1\n3.0\n\n\n4\n18\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n1.0"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#model",
    "href": "posts/blog-3/blog_3.html#model",
    "title": "Blog 3- Auditing Bias",
    "section": "Model",
    "text": "Model\n\n# Setting up the employment problem\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data) \n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]] #taking out employment status (target) and race (so we can look at bias)\n\n(119086, 15)\n(119086,)\n(119086,)\n\n\n\n#train test split\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())]) StandardScaler?Documentation for StandardScalerStandardScaler() DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\nI think a decision tree is a great choice for my model because it helps capture the relationships between demographic factors—like age, education, and race—and the predicted employment status. It’s also really interpretable, so I can see exactly which features are driving false positives and false negatives, which is important for understanding any biases in prediction across different racial groups. Plus, since decision trees handle categorical data well, they work nicely with my dataset, where race is a key variable.\n\n'''\nIn this chunk, I perform a grid search to find the optimal maxmimum depth for my decsision tree model. \nI calculate the best model based on the best maxmimum depth\n'''\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"decisiontreeclassifier__max_depth\":[3, 5, 10, 15] #Testing different depths\n}\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\")  #performing grid search\ngrid_search.fit(X_train, y_train) #training model for each hyper parameter\n\nprint(\"Best maximum depth:\", grid_search.best_params_[\"decisiontreeclassifier__max_depth\"])\nprint(\"Best score:\", grid_search.best_score_)\nbest_model = grid_search.best_estimator_\n\nBest maximum depth: 10\nBest score: 0.8343620377647429"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#data-reading-visualization",
    "href": "posts/blog-3/blog_3.html#data-reading-visualization",
    "title": "Blog 3- Auditing Bias",
    "section": "Data Reading / Visualization",
    "text": "Data Reading / Visualization\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf[\"ESR\"] = (df[\"label\"] == True).astype(int) #creating numeric values for employment status\ndf[\"gender\"]= df[\"SEX\"].map({1: \"Male\", 2: \"Female\"}) #creating categorical labels for gender\n\n\n#Making race into a categorical variable\nrace_map={\n    1 : \"White\",\n    2 : \"Black\",\n    3 : \"American Indigenous\",\n    4 : \"Alaskan Native\",\n    5 : \"American Indigenous, Alaskan Native tribe specified\",\n    6 : \"Asian\",\n    7 : \"Native Hawaiian or Pacific Islander\",\n    8 : \"Some other race\",\n    9 : \"Two or more races\"\n}\ndf[\"race\"] = df[\"group\"].map(race_map) #turning race into a categorical variable\n\n\ntotal_people = len(df)\nproportion_of_people_employed= df[\"ESR\"].mean()\nemployed_by_race= df.groupby([\"race\", \"gender\"])[\"ESR\"].mean().reset_index()\npeople_per_race = df[\"race\"].value_counts()\n\nprint(total_people)\nprint(proportion_of_people_employed)\nprint(people_per_race)\n\n95268\n0.46178150060880885\nrace\nWhite                                                  81427\nBlack                                                   8765\nTwo or more races                                       2399\nAsian                                                   1817\nSome other race                                          654\nAmerican Indigenous                                      119\nAmerican Indigenous, Alaskan Native tribe specified       62\nNative Hawaiian or Pacific Islander                       24\nAlaskan Native                                             1\nName: count, dtype: int64\n\n\n\nemployed_by_race\n\n\n\n\n\n\n\n\nrace\ngender\nESR\n\n\n\n\n0\nAlaskan Native\nFemale\n0.000000\n\n\n1\nAmerican Indigenous\nFemale\n0.447761\n\n\n2\nAmerican Indigenous\nMale\n0.461538\n\n\n3\nAmerican Indigenous, Alaskan Native tribe spec...\nFemale\n0.260870\n\n\n4\nAmerican Indigenous, Alaskan Native tribe spec...\nMale\n0.307692\n\n\n5\nAsian\nFemale\n0.460300\n\n\n6\nAsian\nMale\n0.568362\n\n\n7\nBlack\nFemale\n0.421274\n\n\n8\nBlack\nMale\n0.371907\n\n\n9\nNative Hawaiian or Pacific Islander\nFemale\n0.583333\n\n\n10\nNative Hawaiian or Pacific Islander\nMale\n0.583333\n\n\n11\nSome other race\nFemale\n0.433657\n\n\n12\nSome other race\nMale\n0.449275\n\n\n13\nTwo or more races\nFemale\n0.287490\n\n\n14\nTwo or more races\nMale\n0.300336\n\n\n15\nWhite\nFemale\n0.441879\n\n\n16\nWhite\nMale\n0.504908\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.barplot(data= employed_by_race, x=\"race\", y=\"ESR\", hue=\"gender\", palette={\"Male\": \"blue\", \"Female\": \"pink\"})\nplt.xticks(rotation=45);\n\n\n\n\n\n\n\n\nHow many individuals are in the data?\n-95268 Individuals are in the data\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\n-46.17% of people are employed\nOf these individuals, how many are in each of the groups?\n-We see that the largest group is white people, followed by Black people, bi-racial people, asians, other races and then indegenous people\nIn each group, what proportion of individuals have target label equal to 1?\n-See chart “employed_by_race”, of the categories with meaningful sample sizes (n &gt; 100), we see that Asian people have the highest employment rate (51%), followed by white people (47%), and the lowest rates of employment by race are bi-racial people and American indegenous.\nCheck for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex. This might be a good opportunity to use a visualization such as a bar chart, e.g. via the seaborn package.\n-Looking at employment by race, we can see that on average, men are more employed than women. The only racial group where women are more employed is Black people, where 42% of women are employed and 37% of men."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#model-audit",
    "href": "posts/blog-3/blog_3.html#model-audit",
    "title": "Blog 3- Auditing Bias",
    "section": "Model Audit",
    "text": "Model Audit\n\nOverall Audit\n\n#creating a testing dataframe\ndf_test = pd.DataFrame(X_test, columns = features_to_use) #pulling in the testing data\ny_hat = best_model.predict(X_test)\ndf_test[\"group\"] = group_test #adding back in race\ndf_test[\"label\"] = y_test #adding in employment status\ndf_test[\"ESR\"] = y_test.astype(int)\ndf_test[\"y_hat\"] = y_hat.astype(int) #adding in y_hat\ndf_test[\"race\"] = df_test[\"group\"].map(race_map) #making the race variable categorical\ndf_test.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\nESR\ny_hat\nrace\n\n\n\n\n0\n41.0\n16.0\n1.0\n1.0\n2.0\n0.0\n4.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n1\n1\nWhite\n\n\n1\n43.0\n21.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n0\n1\nWhite\n\n\n2\n47.0\n22.0\n5.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n1\n1\nWhite\n\n\n3\n78.0\n16.0\n2.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n0\n0\nWhite\n\n\n4\n13.0\n10.0\n5.0\n2.0\n2.0\n1.0\n1.0\n1.0\n0.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n0\n0\nWhite\n\n\n\n\n\n\n\n\n#Overall Accuracy\nprint(\"Accuracy: \", (y_hat == y_test).mean())\n\nAccuracy:  0.8322277269292132\n\n\n\n#Positive Predictive Value\nTP = np.sum((y_hat == 1) & (y_test == 1)) \nFP = np.sum((y_hat == 1) & (y_test == 0))\nTN = np.sum((y_hat == 0) & (y_test == 0))\nFN = np.sum((y_hat == 0) & (y_test == 1))\n\nPPV = TP / (TP + FP) #PPV formula\nprint(\"Positive Predictive Value: \", PPV)\n\nPositive Predictive Value:  0.8027530928733229\n\n\n\n#FPR and FNR rates\nFPR = FP / (FP + TN) \nFNR = FN / (FN + TP)\nprint(\"False Positive Rate: \", FPR)\nprint(\"False Negative Rate: \", FNR)\n\nFalse Positive Rate:  0.17588564325668116\nFalse Negative Rate:  0.1582313173762105\n\n\nWe see that the model has an overal accuracy of 83%, and a lower positive predictive value of 80%. We also see that the false negative rate (15%) is lower than the false positive rate (17%), so the model is more likely to predict someone is employed when they aren’t than prediciting someone is employed when they are.\n\n\nSubgroup accuracy\n\n#Accuracy on each sub group\ndf_test[\"correct_prediction\"] = (df_test[\"ESR\"] == df_test[\"y_hat\"]).astype(int) #sorting correct predictions as a column\nsubgroup_acc= df_test.groupby(\"race\")[\"correct_prediction\"].mean().reset_index() #sorting by race\nsubgroup_acc\n\n\n\n\n\n\n\n\nrace\ncorrect_prediction\n\n\n\n\n0\nAmerican Indigenous\n0.878788\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n0.866667\n\n\n2\nAsian\n0.808279\n\n\n3\nBlack\n0.813702\n\n\n4\nNative Hawaiian or Pacific Islander\n1.000000\n\n\n5\nSome other race\n0.856250\n\n\n6\nTwo or more races\n0.885099\n\n\n7\nWhite\n0.832942\n\n\n\n\n\n\n\n\n#PPV on each subgroup\nwarnings.simplefilter(\"ignore\")\n\ndef compute_ppv(group):\n    '''\n    computes the ppv of each group\n    args: group- race\n    returns: the ppv by subgroup\n    '''\n    tp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 1)).sum()\n    fp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 0)).sum()\n    ppv = tp / (tp + fp)\n    return pd.Series({\"PPV\": ppv})\n\n\nsubgroup_ppv = df_test.groupby(\"race\").apply(compute_ppv).reset_index() #applying ppv function by subgroup\n\nsubgroup_ppv\n\n\n\n\n\n\n\n\nrace\nPPV\n\n\n\n\n0\nAmerican Indigenous\n0.833333\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n1.000000\n\n\n2\nAsian\n0.786260\n\n\n3\nBlack\n0.742424\n\n\n4\nNative Hawaiian or Pacific Islander\n1.000000\n\n\n5\nSome other race\n0.775000\n\n\n6\nTwo or more races\n0.775862\n\n\n7\nWhite\n0.809267\n\n\n\n\n\n\n\n\n#FPR and FNR by subgroup\nwarnings.simplefilter(\"ignore\")\ndef calculate_fpr_fnr(group):\n    '''\n    Calcuates the false posititve and negative rates by group\n    Args: group- in our case rae\n    returns: the false positive and negative rates for each group\n    '''\n    fp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 0)).sum() #false positive calc\n    fn = ((group[\"y_hat\"] == 0) & (group[\"ESR\"] == 1)).sum() #false negative calc\n    tp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 1)).sum() #true positive calc\n    tn = ((group[\"y_hat\"] == 0) & (group[\"ESR\"] == 0)).sum() #true negative calc\n\n    fpr = fp / (fp + tn); #fpr rate\n    fnr = fn / (fn + tp); #fnr rate\n\n    return pd.Series({\"FPR\": fpr, \"FNR\": fnr})\n\nsubgroup_fpr_fnr = df_test.groupby(\"race\").apply(calculate_fpr_fnr).reset_index() #applying function\n\nsubgroup_fpr_fnr\n\n\n\n\n\n\n\n\nrace\nFPR\nFNR\n\n\n\n\n0\nAmerican Indigenous\n0.176471\n0.062500\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n0.000000\n0.400000\n\n\n2\nAsian\n0.253394\n0.134454\n\n\n3\nBlack\n0.185070\n0.188166\n\n\n4\nNative Hawaiian or Pacific Islander\nNaN\n0.000000\n\n\n5\nSome other race\n0.193548\n0.074627\n\n\n6\nTwo or more races\n0.098237\n0.156250\n\n\n7\nWhite\n0.176069\n0.156887\n\n\n\n\n\n\n\nWe see that the model performs decently well across different catgeories of race, especially for races with high numbers of observations. The accuracy for each subgroup is in the low to mid 80%s, which is pretty standard across races and matches the overall. The PPV is lowest for Black people (74%) and much higher for white people (80%), which is concerning, because it means white people overall are being predicted to be employed at higher rates. We also see pretty standard error rate balance across racial groups with large numbers of observations except for Asian and Indigenous people, where the rate of a false positive is much higher than the rate of a false negative.\n\n\nBias Measures\n\n#Is your model approximately calibrated?\ncalibration = df_test.groupby([\"y_hat\", \"race\"])[\"ESR\"].mean().unstack() #grouping y_hat by race and ESR to compare across races for values of y_hat\ncalibration\n\n\n\n\n\n\n\nrace\nAmerican Indigenous\nAmerican Indigenous, Alaskan Native tribe specified\nAsian\nBlack\nNative Hawaiian or Pacific Islander\nSome other race\nTwo or more races\nWhite\n\n\ny_hat\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.066667\n0.166667\n0.162437\n0.131732\nNaN\n0.0625\n0.065274\n0.144361\n\n\n1\n0.833333\n1.000000\n0.786260\n0.742424\n1.0\n0.7750\n0.775862\n0.809267\n\n\n\n\n\n\n\n\n#Does your model satisfy approximate error rate balance?\nwarnings.simplefilter(\"ignore\")\nerror_rates_by_race = df_test.groupby(\"race\").apply(calculate_fpr_fnr).reset_index() #using the FPR and NPR calculation from earlier\nerror_rates_by_race\n\n\n\n\n\n\n\n\nrace\nFPR\nFNR\n\n\n\n\n0\nAmerican Indigenous\n0.176471\n0.062500\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n0.000000\n0.400000\n\n\n2\nAsian\n0.253394\n0.134454\n\n\n3\nBlack\n0.185070\n0.188166\n\n\n4\nNative Hawaiian or Pacific Islander\nNaN\n0.000000\n\n\n5\nSome other race\n0.193548\n0.074627\n\n\n6\nTwo or more races\n0.098237\n0.156250\n\n\n7\nWhite\n0.176069\n0.156887\n\n\n\n\n\n\n\n\n#Does your model satisfy statistical parity?\n\nsHR = 0.5 #threshold\ndf_test[\"score\"] = best_model.predict_proba(X_test)[:, 1] #creating score column\n\ndef compute_high_risk_rate(group):\n    '''\n    Computes a group's rate of unenmployment\n    args: group, in our case, race\n    returns: high_risk/total, the statistical parity for each group\n    '''\n    high_risk = (group[\"score\"] &lt; sHR).sum() #a person is \"high risk\" if the score is higher than the threshold\n    total = len(group)\n    return high_risk / total #creating an average\n\nstat_parity_by_race = df_test.groupby(\"race\").apply(compute_high_risk_rate).reset_index()\nprint(stat_parity_by_race)\n\n                                                race         0\n0                                American Indigenous  0.454545\n1  American Indigenous, Alaskan Native tribe spec...  0.800000\n2                                              Asian  0.427015\n3                                              Black  0.565931\n4                Native Hawaiian or Pacific Islander  0.000000\n5                                    Some other race  0.500000\n6                                  Two or more races  0.687612\n7                                              White  0.509726\n\n\nIn terms of bias measures, we see that the model is approximatley calibrated, with similar means of ESR for predicted values of ESR by race, espeically after taking out the groups that appeared very briefly in the survey data. We find that error rate parity is low for Asians, Indigenous people (higher false positives than false negatives) and bi-racial people (higher false negatives than false positives), but is pretty standard across other racial groups.The model also does a decent job of satisfying statistical parity, with a score of about .5 for races with high numbers of observations. The score however is higher for Black people and bi-racial people, and is lower for white people and Asians, meaning that they are less likely to be predicted as “high risk”.\n\n\nRe-creating the plot\n\ndf_test[\"FP\"] = ((df_test[\"y_hat\"] == 1) & (df_test[\"ESR\"] == 0)).astype(int)\ndf_test[\"FN\"] = ((df_test[\"y_hat\"] == 0) & (df_test[\"ESR\"] == 1)).astype(int)\ndf_test[\"TP\"] = ((df_test[\"y_hat\"] == 1) & (df_test[\"ESR\"] == 1)).astype(int)\ndf_test[\"TN\"] = ((df_test[\"y_hat\"] == 0) & (df_test[\"ESR\"] == 0)).astype(int)\ndf_test[\"FPR\"] = df_test[\"FP\"] / (df_test[\"FP\"] + df_test[\"TN\"]) #False Positive Rate\ndf_test[\"FNR\"] = df_test[\"FN\"] / (df_test[\"FN\"] + df_test[\"TP\"]) # False Negative Rate\nfpr_fnr_by_race = df_test.groupby(\"race\")[[\"FPR\", \"FNR\"]].mean().reset_index()\n\n\nwarnings.simplefilter(\"ignore\")\npalette = sns.color_palette(\"Set2\", n_colors=len(fpr_fnr_by_race))\n\ng = sns.lmplot(x=\"FP\", y=\"FN\", hue=\"race\", data=df_test, # Creating an lm plot using seaborn\n                y_jitter=.02, truncate=False)\ng.set(xlim=(0, 1), ylim=(0, .2))\n\nfor i, row in fpr_fnr_by_race.iterrows():\n    race_color = palette[i]  #Use the color corresponding to each race\n    plt.scatter(x=row[\"FPR\"], y=row[\"FNR\"], s=50, color=race_color, marker=\"o\", edgecolors=\"black\") #adding in the FPR, FNR rates by race\n    plt.text(row[\"FPR\"], row[\"FNR\"], f'{row[\"race\"]}', fontsize=9, ha='right', color=race_color) # labeling points\n\n# Display the plot\nplt.show()\n\nposx and posy should be finite values\nposx and posy should be finite values\n\n\n\n\n\n\n\n\n\nIf we desired to tune our classifier threshold so that the false positive rates were equal between groups, how much would we need to change the false negative rate? If we adjust the classifier threshold to equalize false positive (FP) rates across all groups, we would likely see false negative (FN) rates shift unevenly between groups.From the plot, groups with initially lower FP rates (such as Asian or American Indigenous) would require a lower threshold, increasing their FP rate while also increasing their FN rate. Conversely, groups with higher FP rates (such as Some Other Race or Two or More Races) would need a higher threshold, reducing their FP rate but also lowering their FN rate. In general, equalizing FP rates would widen the gap in FN rates, potentially exacerbating disparities in under-recognition for some groups."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#discussion",
    "href": "posts/blog-3/blog_3.html#discussion",
    "title": "Blog 3- Auditing Bias",
    "section": "Discussion",
    "text": "Discussion\nEmployers looking to hire people for jobs where they want little turnover would like to use this model, it would help them understand the qualities that make a perspecitve employee retain employment for longer. Large companies who hire a lot of people at once would also benefit from using algorithimic descision making to hire people. So large companies like manufacturing or mass retaliers could benefit from a speedup in their hiring processes. From my bias audit, the impact of deploying my model could be prepetuations of racism in society, as Black people and bi-racial people are less expected to be employed than white and Asian people. Deploying this model could lead to discriminitory practices implemented in the workplace through hiring, unless it is corrected by mediating the bias in the model through hiring caps. I feel that there is strong error rate bias which disproportionatley effects bi-racial and Black people, as they are more likely to be predicted as unemployed when they are employed than employed when they are not. The other thing that would make me nervous about deploying this model is that a lot of these attributes in the dataframe are out of a person’s control, and are other forms of demographic information. Using a model like this to help in the hiring process may speed up hiring processes and eliminate some discrimination through removing a singluar judge (one hiring manager could have implicit biases), but it also may remove some benfits of merrit based hiring. People who are smart and well-suited for a job may not get a job because of demographic factors outside of their control."
  },
  {
    "objectID": "posts/blog-2/blog-2.html",
    "href": "posts/blog-2/blog-2.html",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This blog post analyzes fairness in loan approval processes. I first make a model given the training data and use the coefficents to create a score. I compare that score to a threshold to deterimine if the bank should or should not make the loan based on the training attributes. I find an average bank profit of 1820 after testing out different training features. Despite this being the optimal value, we find that the bank is likely to discriminate against people requesting medical loans, and people with lower incomes. Although these were not explicit features in the model, the model still learned that loaning to the above groups generated more risk that the bank wasn’t willing to take on."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#abstract",
    "href": "posts/blog-2/blog-2.html#abstract",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This blog post analyzes fairness in loan approval processes. I first make a model given the training data and use the coefficents to create a score. I compare that score to a threshold to deterimine if the bank should or should not make the loan based on the training attributes. I find an average bank profit of 1820 after testing out different training features. Despite this being the optimal value, we find that the bank is likely to discriminate against people requesting medical loans, and people with lower incomes. Although these were not explicit features in the model, the model still learned that loaning to the above groups generated more risk that the bank wasn’t willing to take on."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#setup",
    "href": "posts/blog-2/blog-2.html#setup",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train[\"loan_int_rate\"]= (df_train[\"loan_int_rate\"])/100 #making the loan rate into a percentage\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n0.0991\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nIn this chunk, I set up the data, reset the interest rate so it’s a percentage out of 100 and view it."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#explore",
    "href": "posts/blog-2/blog-2.html#explore",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Explore",
    "text": "Explore\n\n# data display setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n\ndata = df_train\n\n#Creating plot\nsns.displot(data=data, x=\"person_age\", hue=\"loan_intent\", multiple=\"stack\")\n\n# Adjusting x-axis limits\nplt.xlim(20, 60)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the relationship between a person’s age and their loan intent. We can see that the most loans are requested by people in their mid-20s, and are mostly for education, with medical aand venture also being top categories. As people get older, they are less likely to want a loan, but the education intent goes down and personal, home improvement and venture loan intents increase.\n\nsns.scatterplot(data= data, x= \"person_income\", y= \"loan_int_rate\", hue = \"loan_status\")\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the people who get approved for loans often have the highest interest rates, which makes sense from the bank’s perspective. A higher interest rate means the bank is making more money, which offsets the risk they take on by offering the loan. On the other hand, It’s suprising that people with higher incomes are often not approved for loans. I would expect people with higher income to be more likely to be approved for a loan, given that they have more proof they can pay. A variable that might effect this outcome is the amount they are asking for. It’s possible that people with high incomes are asking for large loans that the bank doesn’t feel comfortable taking on."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#model",
    "href": "posts/blog-2/blog-2.html#model",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Model",
    "text": "Model\n\n#data cleaning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n#cleaning data\ndf_train= df_train.dropna()\n# econding possible outcome variable\nle = LabelEncoder()\ndf_train[\"loan_intent\"]= le.fit_transform(df_train[\"loan_intent\"]) \n\n\ndf_train.head()\n\n#df_train[\"income_for_age\"]= df_train[\"person_emp_length\"]+ df_train[\"cb_person_cred_hist_length\"]\ncols= [\"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\"]\nX_train = df_train[cols]\nX_train = StandardScaler().fit_transform(X_train)  # Standardize features\ny_train = df_train[\"loan_status\"]\n\nI clean the data by dropping NA values and then create a train dataset with target columns, standardize it and then come up with y values.\n\n#modeling\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nweights= model.coef_.flatten()\nprint(weights)\n\n[-0.59390244  1.26796607  1.03233148]\n\n\nI chose to use logistic regression for this model because the goal is to predict the probability of loan default, which is a binary classification problem. Logistic regression is well-suited for this task as it provides interpretable probability outputs, allowing for clear threshold-based decision-making.\n\ndef linear_score(X, w):\n  \"\"\"\n  Calculates the linear score between features and weights\n  \"\"\"\n  return X@w # or np.dot(X, w)\n\ndf_train[\"score\"]= linear_score(X_train, weights)\n\ndf_train[\"score\"].head()\nprint(df_train[\"score\"].max())\nprint(df_train[\"score\"].min())\nprint(df_train[\"score\"].mean())\n\n8.291911080319613\n-4.45964619067289\n3.300377486570684e-16\n\n\nHere I calculate the linear score between the features (in the columns of X_train) and the weights calculated from the model."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#find-a-threshold",
    "href": "posts/blog-2/blog-2.html#find-a-threshold",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Find a Threshold",
    "text": "Find a Threshold\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n                      \nimport numpy as np\ndef profit(data, t):\n    \"\"\"\n    Calculates the profit the bank makes on a given loan\n\n    Args: \n    data: the dataframe containting the loan requesters\n    t: the threshold for accepting a loan\n\n    returns: the updated dataframe with a profit column\n    \"\"\"\n\n    #create binary yes/no values from scores based on t\n    y_pred = (data[\"score\"] &gt; t).astype(int)\n    data[\"y_pred\"]= y_pred\n    data[\"bank_profit\"] = 0.0\n    #print(y_pred, y_scores)\n    i00 = (data[\"loan_status\"] == 0)& (y_pred == 0)\n    i01 = (data[\"loan_status\"] == 0) & (y_pred == 1)\n    i10 = (data[\"loan_status\"] == 1) & (y_pred == 0)\n    i11 = (data[\"loan_status\"] == 1) &(y_pred == 1)\n    #sum up the bank profit based on the predicted values of weather or not someone will default\n    data[\"bank_profit\"][i00] = data[\"loan_amnt\"][i00] * (1 + 0.25 * data[\"loan_int_rate\"][i00]) ** 10 - data[\"loan_amnt\"][i00] # best case\n    data[\"bank_profit\"][i01] = np.nan\n    data[\"bank_profit\"][i10] = data[\"loan_amnt\"][i10] * (1 + 0.25 * data[\"loan_int_rate\"][i10]) ** 3 -1.7 * data[\"loan_amnt\"][i10] # worst case\n    data[\"bank_profit\"][i11] = np.nan\n\n    return data\n\n\nthresholds = np.linspace(-.5, 2, 101)  # Test values between -.5 and 2\nbank_profits = [profit(df_train, t)[\"bank_profit\"].mean() for t in thresholds] # calculates the average profit per borrower for each t\nprint(sum(bank_profits))\n\n\nt_star = thresholds[np.argmax(bank_profits)] #finds the argmax of the bank profits in the bank profit list\n\nprint(t_star)\n\nprofit(df_train, t_star).head()\n\n\n168930.56163829993\n0.050000000000000044\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nscore\ny_pred\nbank_profit\n\n\n\n\n1\n27\n98000\nRENT\n3.0\n1\nC\n11750\n0.1347\n0\n0.12\nY\n6\n-0.008090\n0\n4613.567568\n\n\n2\n22\n36996\nRENT\n5.0\n1\nA\n10000\n0.0751\n0\n0.27\nN\n4\n0.049639\n0\n2044.334031\n\n\n3\n24\n26000\nRENT\n2.0\n3\nC\n1325\n0.1287\n1\n0.05\nN\n4\n-0.055660\n0\n-795.445199\n\n\n4\n29\n53004\nMORTGAGE\n2.0\n2\nA\n15000\n0.0963\n0\n0.28\nN\n10\n0.375078\n1\nNaN\n\n\n6\n21\n21700\nRENT\n2.0\n2\nD\n5500\n0.1491\n1\n0.25\nN\n2\n2.595504\n1\nNaN\n\n\n\n\n\n\n\nHere I generate a profit function to calculate the best value that the bank should accept loans at as a threshold value. I do this by creating index markers (i00, i01, i10 and i11) to denote a persons loan status (default or not) and their y_pred, weather or not the bank gives them a loan. I then calculate the profitability using these markers and the equations in class. If a person never recieved a loan, the bank never recieves anything, hence the NaN. I chose to use NaN instead of 0 because it will be easier to calculate the average profit per borrower only including the people who borrowed money.\nI then run the profit function for a range of values using np.linspace to find the optimal value of t where average profit per borrower is the highest.\n\n#plotting the graph\n\nthresholds = np.linspace(-.5, 2, 101)\nprofits = [profit(df_train, t)[\"bank_profit\"].mean() for t in thresholds]\n\n# Find the best threshold\nbest_threshold = thresholds[np.argmax(profits)]\nbest_profit = max(profits)\n\n# Plot using Seaborn\nsns.lineplot(x=thresholds, y=profits)\n\n# Add vertical line for the best threshold\nplt.axvline(best_threshold, linestyle=\"--\", color=\"gray\")\n\n# Labels and title\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Expected Profit per Borrower\")\nplt.title(f\"Max Profit: {best_profit:.3f} at Threshold {best_threshold:.3f}\")\n\nplt.show()\n\n\n\n\n\n\n\n\nHere we see that the optimal value for t= 0.05, with max profit of $1820 per borrower. We can see a steady climb up to the threshold with increasing values of t and then a sharper droppoff as the bank takes on large sums of risk. An addition to the model would be evaluating the opportunity cost of rejecting a loan with someone who would pay it back to better reflect the downside to being risk-adverse"
  },
  {
    "objectID": "posts/blog-2/blog-2.html#evaluate-from-the-banks-perspective",
    "href": "posts/blog-2/blog-2.html#evaluate-from-the-banks-perspective",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Evaluate from the bank’s perspective",
    "text": "Evaluate from the bank’s perspective\n\n# Setup for test/clean the testing data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test[\"loan_int_rate\"]= (df_test[\"loan_int_rate\"])/100\ndf_test= df_test.dropna()\n\n\nX_test= df_test[cols]\nX_test = StandardScaler().fit_transform(X_test)\ny_test= df_test[\"loan_status\"]\ndf_test[\"score\"]= linear_score(X_test, weights)\nprint(t_star)\nprint(profit(df_test, t_star)[\"bank_profit\"].mean().astype(int))\ndf_test[\"score\"].head()\n\n0.050000000000000044\n1743\n\n\n0    0.486485\n1    0.999932\n2   -0.185437\n3    0.520230\n4    0.731585\nName: score, dtype: float64\n\n\nI create X and Y test here and run my profit function on it to see what the testing data looks like.\nI’m getting a similar average bank profit with the testing data, which makes sense, but at $1615, it is 200 dollars lower than that of the training data. Maybe the characteristics in the training data I used aren’t as apparent in the testing data, and maybe different patterns or stronger. It could also be that the testing data has less opporunity for bank income in the first place."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#evaluate-from-the-borrowers-perspective",
    "href": "posts/blog-2/blog-2.html#evaluate-from-the-borrowers-perspective",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Evaluate from the borrower’s perspective",
    "text": "Evaluate from the borrower’s perspective\n\nage_groups = [18, 25, 35, 50, 65, 100]\ndf_test[\"age_group\"] = pd.cut(df_test[\"person_age\"], bins=age_groups, labels=[\"18-25\", \"26-35\", \"36-50\", \"51-65\", \"65+\"])\ndf_test.groupby(\"age_group\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nage_group\ny_pred\nloan_status\n\n\n\n\n0\n18-25\n0.490789\n0.231024\n\n\n1\n26-35\n0.445140\n0.216938\n\n\n2\n36-50\n0.440767\n0.202091\n\n\n3\n51-65\n0.487805\n0.317073\n\n\n4\n65+\n0.600000\n0.200000\n\n\n\n\n\n\n\nYounger people are less likley to recieve loans, and they are also the second most likely group to default on their loans. Surpisingly, people aged 51-65 are more likely to recieve loans, but also the most likely to default on them. Given their age, I would assume that they get the benefit of the doubt and are given loans more frequently.\n\ndf_test.groupby(\"loan_intent\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nloan_intent\ny_pred\nloan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.461283\n0.287611\n\n\n1\nEDUCATION\n0.476190\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.435065\n0.250000\n\n\n3\nMEDICAL\n0.486486\n0.284250\n\n\n4\nPERSONAL\n0.470942\n0.220441\n\n\n5\nVENTURE\n0.454357\n0.146266\n\n\n\n\n\n\n\nMedical loans aren’t super likely to given, probably because they have a higher default rate of 28%. Venture and education are most likely to be given, and both have low default rates. This is probably because both loans are seen as an investment into future great economic opportunity.\n\nincome_bins = [0, 30000, 60000, 100000, np.inf]\nincome_labels = [\"&lt;30K\", \"30K-60K\", \"60K-100K\", \"100K+\"]\ndf_test[\"income_group\"] = pd.cut(df_test[\"person_income\"], bins=income_bins, labels=income_labels)\n\ndf_test.groupby(\"income_group\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nincome_group\ny_pred\nloan_status\n\n\n\n\n0\n&lt;30K\n0.771242\n0.460131\n\n\n1\n30K-60K\n0.557899\n0.242340\n\n\n2\n60K-100K\n0.342538\n0.136898\n\n\n3\n100K+\n0.133156\n0.110519\n\n\n\n\n\n\n\nAs income increases, people are more likely to be approved for loans and less likely to default. This makes sense, as people with greater income have more stability, and are more likely to have the capital to pay off a loan, even if their purpose in getting it didn’t pay off."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#discussion",
    "href": "posts/blog-2/blog-2.html#discussion",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Discussion",
    "text": "Discussion\nFairness means that people are given just treatment without discrimination or bias. Therefore, this blog post analyzes questions of fairness by looking at what makes someone elligible for a loan. One of the results was that people requesting medical loans are more likely to default, thereby reducing their chances of recieving the loan. I think that this is unfair, but not from the bank’s perspecitive. As a player in a capitalist society, the bank’s responsibility is to increase profit for their shareholders. Giving out loans with higher risk of default shouldn’t be their responsibility, as it wouldn’t be fair to let the bank take the responsibility for high costs of medical care. I think that the burden of responsibility lies with the government. Healthcare shouldn’t be a for-profit industry, as profitability and health goals often do not align. Instead, what would be fair is if the government offered more healthcare benefits so people who needed medical procedures wouldn’t have to take out a loan."
  },
  {
    "objectID": "posts/blog-4/blog-4.html",
    "href": "posts/blog-4/blog-4.html",
    "title": "Blog 4- Replication Study",
    "section": "",
    "text": "In this paper, I reproduce aspects of Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations by Obermeyer et al. I find that Black people are rated at lower risk by an algorithim to help hospitals prioritize patient care than white people with the same number of chronic conditions. To understand why this is the case, I use a model to understand the role cost plays. I find that Black people spend around 75% of what white people spend for the same medical issues, which is why risk score is higher."
  },
  {
    "objectID": "posts/blog-4/blog-4.html#abstract",
    "href": "posts/blog-4/blog-4.html#abstract",
    "title": "Blog 4- Replication Study",
    "section": "",
    "text": "In this paper, I reproduce aspects of Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations by Obermeyer et al. I find that Black people are rated at lower risk by an algorithim to help hospitals prioritize patient care than white people with the same number of chronic conditions. To understand why this is the case, I use a model to understand the role cost plays. I find that Black people spend around 75% of what white people spend for the same medical issues, which is why risk score is higher."
  },
  {
    "objectID": "posts/blog-4/blog-4.html#setup",
    "href": "posts/blog-4/blog-4.html#setup",
    "title": "Blog 4- Replication Study",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\npd.options.display.max_columns = None# Show all columns\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\ndem_female\ndem_age_band_18-24_tm1\ndem_age_band_25-34_tm1\ndem_age_band_35-44_tm1\ndem_age_band_45-54_tm1\ndem_age_band_55-64_tm1\ndem_age_band_65-74_tm1\ndem_age_band_75+_tm1\nalcohol_elixhauser_tm1\nanemia_elixhauser_tm1\narrhythmia_elixhauser_tm1\narthritis_elixhauser_tm1\nbloodlossanemia_elixhauser_tm1\ncoagulopathy_elixhauser_tm1\ncompdiabetes_elixhauser_tm1\ndepression_elixhauser_tm1\ndrugabuse_elixhauser_tm1\nelectrolytes_elixhauser_tm1\nhypertension_elixhauser_tm1\nhypothyroid_elixhauser_tm1\nliver_elixhauser_tm1\nneurodegen_elixhauser_tm1\nobesity_elixhauser_tm1\nparalysis_elixhauser_tm1\npsychosis_elixhauser_tm1\npulmcirc_elixhauser_tm1\npvd_elixhauser_tm1\nrenal_elixhauser_tm1\nuncompdiabetes_elixhauser_tm1\nvalvulardz_elixhauser_tm1\nwtloss_elixhauser_tm1\ncerebrovasculardz_romano_tm1\nchf_romano_tm1\ndementia_romano_tm1\nhemiplegia_romano_tm1\nhivaids_romano_tm1\nmetastatic_romano_tm1\nmyocardialinfarct_romano_tm1\npulmonarydz_romano_tm1\ntumor_romano_tm1\nulcer_romano_tm1\ncost_dialysis_tm1\ncost_emergency_tm1\ncost_home_health_tm1\ncost_ip_medical_tm1\ncost_ip_surgical_tm1\ncost_laboratory_tm1\ncost_op_primary_care_tm1\ncost_op_specialists_tm1\ncost_op_surgery_tm1\ncost_other_tm1\ncost_pharmacy_tm1\ncost_physical_therapy_tm1\ncost_radiology_tm1\nlasix_dose_count_tm1\nlasix_min_daily_dose_tm1\nlasix_mean_daily_dose_tm1\nlasix_max_daily_dose_tm1\ncre_tests_tm1\ncrp_tests_tm1\nesr_tests_tm1\nghba1c_tests_tm1\nhct_tests_tm1\nldl_tests_tm1\nnt_bnp_tests_tm1\nsodium_tests_tm1\ntrig_tests_tm1\ncre_min-low_tm1\ncre_min-high_tm1\ncre_min-normal_tm1\ncre_mean-low_tm1\ncre_mean-high_tm1\ncre_mean-normal_tm1\ncre_max-low_tm1\ncre_max-high_tm1\ncre_max-normal_tm1\ncrp_min-low_tm1\ncrp_min-high_tm1\ncrp_min-normal_tm1\ncrp_mean-low_tm1\ncrp_mean-high_tm1\ncrp_mean-normal_tm1\ncrp_max-low_tm1\ncrp_max-high_tm1\ncrp_max-normal_tm1\nesr_min-low_tm1\nesr_min-high_tm1\nesr_min-normal_tm1\nesr_mean-low_tm1\nesr_mean-high_tm1\nesr_mean-normal_tm1\nesr_max-low_tm1\nesr_max-high_tm1\nesr_max-normal_tm1\nghba1c_min-low_tm1\nghba1c_min-high_tm1\nghba1c_min-normal_tm1\nghba1c_mean-low_tm1\nghba1c_mean-high_tm1\nghba1c_mean-normal_tm1\nghba1c_max-low_tm1\nghba1c_max-high_tm1\nghba1c_max-normal_tm1\nhct_min-low_tm1\nhct_min-high_tm1\nhct_min-normal_tm1\nhct_mean-low_tm1\nhct_mean-high_tm1\nhct_mean-normal_tm1\nhct_max-low_tm1\nhct_max-high_tm1\nhct_max-normal_tm1\nldl_min-low_tm1\nldl_min-high_tm1\nldl_min-normal_tm1\nldl-mean-low_tm1\nldl-mean-high_tm1\nldl-mean-normal_tm1\nldl_max-low_tm1\nldl_max-high_tm1\nldl_max-normal_tm1\nnt_bnp_min-low_tm1\nnt_bnp_min-high_tm1\nnt_bnp_min-normal_tm1\nnt_bnp_mean-low_tm1\nnt_bnp_mean-high_tm1\nnt_bnp_mean-normal_tm1\nnt_bnp_max-low_tm1\nnt_bnp_max-high_tm1\nnt_bnp_max-normal_tm1\nsodium_min-low_tm1\nsodium_min-high_tm1\nsodium_min-normal_tm1\nsodium_mean-low_tm1\nsodium_mean-high_tm1\nsodium_mean-normal_tm1\nsodium_max-low_tm1\nsodium_max-high_tm1\nsodium_max-normal_tm1\ntrig_min-low_tm1\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n90.0\n380.0\n640.0\n3370.0\n700.0\n0.0\n0.0\n0.0\n0\n0\n0.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n110.0\n0.0\n0.0\n0.0\n200.0\n1480.0\n2130.0\n1940.0\n6200.0\n0.0\n0.0\n300.0\n0\n0\n0.0\n0\n1\n0\n0\n0\n2\n1\n0\n1\n1\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0\n0\n0.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n270.0\n490.0\n90.0\n0.0\n0.0\n0.0\n0.0\n0\n0\n0.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n180.0\n390.0\n330.0\n90.0\n200.0\n0.0\n0.0\n0.0\n0\n0\n0.0\n0\n4\n0\n0\n0\n2\n0\n0\n3\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1"
  },
  {
    "objectID": "posts/blog-4/blog-4.html#plot-re-creation",
    "href": "posts/blog-4/blog-4.html#plot-re-creation",
    "title": "Blog 4- Replication Study",
    "section": "Plot Re-Creation",
    "text": "Plot Re-Creation\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf[\"percentile\"] = pd.qcut(df['risk_score_t'], q=100, labels= False) #risk percentiles\n\nmarkers = {\"white\": \"x\", \"black\": \"o\"} #markers for race\ncolors = {\"white\": \"green\", \"black\": \"blue\"}\n\nsns.set_style(\"whitegrid\")\n\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n#looping through all people and sorting based on gender\nfor i, female in enumerate([0, 1]): #female = 1\n    ax = axes[i]\n    #loop through all races\n    for race in df[\"race\"].unique():\n        subset = df[(df[\"dem_female\"] == female) & (df[\"race\"] == race)]\n        grouped = subset.groupby(\"percentile\")[\"gagne_sum_t\"].mean().reset_index()\n        sns.scatterplot(\n            data=grouped, \n            x=\"gagne_sum_t\", \n            y=\"percentile\", \n            marker=markers[race], \n            color = colors[race],\n            label=f\"{race.capitalize()} Patients\", \n            ax=ax\n        )\n    ax.set_title(f\"{'female' if female == 1 else 'male'} patients\")\n    ax.set_xlabel(\"Mean Chronic Conditions\")\n    if i == 0:\n        ax.set_ylabel(\"Risk Score (Percentile)\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHere we can see that white patients are more likely to be in a higher risk score percentile even if they have the same mean chronic conditions. We see the gap in between races widens as mean chronic conditions increase for both genders, espeically women. This means that there are other factors coming from ommited variable bias at play. These factors could be related to racism and the formula for calculating their risk score, which could include how much money patients are spending.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmarkers = {\"black\": \"o\", \"white\": \"X\"} #markers and colors for race\ncolors = {\"black\": \"navy\", \"white\": \"green\"}\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor i, metric in enumerate([\"percentile\", \"gagne_sum_t\"]): #looping through each of the two plots\n    ax = axes[i]\n\n    for race in df[\"race\"].unique(): #loop through races\n        subset = df[df[\"race\"] == race] #creating a subset of race for each race\n        \n        group = subset.groupby(metric)[\"cost_t\"].mean().reset_index() #grouping for each race by metric\n\n        sns.scatterplot(\n            data=group,\n            x=metric,\n            y=\"cost_t\",\n            marker=markers[race],\n            color=colors[race],\n            label=f\"{race.capitalize()} Patients\",\n            ax=ax\n        )\n\n\n    ax.set_yscale(\"log\") #log scale\n    ax.set_ylabel(\"Total medical expenditure\")\n    ax.set_xlabel(\"Percentile risk score\" if metric == \"percentile\" else \"Number of Chronic Illnesses\")\n\n    ax.legend(title=\"Race\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow we are pulling in medical expenditure (on a logarithmic scale) into the equation. We can see that the average black person at the same metric (risk percentile or chronic illnesses) spends less on medical expenses than the average white person. This is especially true as the percentile risk and the number of chronic conditions are normalized under a certian threshold."
  },
  {
    "objectID": "posts/blog-4/blog-4.html#data-prep",
    "href": "posts/blog-4/blog-4.html#data-prep",
    "title": "Blog 4- Replication Study",
    "section": "Data Prep",
    "text": "Data Prep\n\ncount = (df['gagne_sum_t'] &lt;= 5).sum()\ncount/df.shape[0]\n\nnp.float64(0.9553952115447688)\n\n\n95% of people have 5 or less chronic conditions, so it makes sense to just focus on these people to get a better sense of the true population we want to cover.\n\ndf_cost = df[df[\"cost_t\"] != 0] # creating a dataframe where costs aren't equal to 0\ndf_cost[\"log_cost\"]= np.log(df[\"cost_t\"]) #taking log of costs\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/var/folders/gx/d6zl9l4908q7p_vk4j3x06x00000gn/T/ipykernel_40052/212932897.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cost[\"log_cost\"]= np.log(df[\"cost_t\"])\n\n\n\ndf_cost[\"race_d\"]= df[\"race\"].map({\"white\": 0, \"black\": 1}) #creating a binary variable for race\n\n/var/folders/gx/d6zl9l4908q7p_vk4j3x06x00000gn/T/ipykernel_40052/3074851344.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cost[\"race_d\"]= df[\"race\"].map({\"white\": 0, \"black\": 1})"
  },
  {
    "objectID": "posts/blog-4/blog-4.html#model",
    "href": "posts/blog-4/blog-4.html#model",
    "title": "Blog 4- Replication Study",
    "section": "Model",
    "text": "Model\n\n#setting up modeling dataframes\nX= df_cost[[\"race_d\", \"gagne_sum_t\"]] \ny= df_cost[\"log_cost\"]\n\n\ndef add_polynomial_features(X, degree): #adding polynomial features function\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\n\nfrom sklearn.linear_model import Ridge\nk = [-4, -3, -2, -1, 0, 1, 2, 3, 4] #regularization stenghts\n\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom sklearn.model_selection import cross_val_score\nbest_score= 0\nresult= []\nfor i in k: # looping through regularization strengths\n    for j in range(1, 11): #looping through polynomial degrees\n        alpha = 10 ** k[i]\n        newX= add_polynomial_features(X, j) #adding polynomial features to x\n        LR = Ridge(alpha = alpha) \n        score_p = cross_val_score(LR, newX, y, cv = 5, scoring='neg_mean_squared_error').mean() # calculating the score by the mean of the squared error\n        score= -score_p #negating the score\n        result.append({\"reg_strength_coef\": i, \n                        \"reg_strength\": alpha,\n                        \"degree\" : j,\n                        \"mse\": score}) #storing the results\n\ndf_result = pd.DataFrame(result) # creating a df from the results\ndf_result = df_result.sort_values(\"mse\") # sorting by mean squared error\nbest = round(df_result.iloc[0],2) #pulling out the highest values\nbest\n\n\n\nreg_strength_coef     4.00\nreg_strength          1.00\ndegree               10.00\nmse                   1.51\nName: 89, dtype: float64\n\n\n\nmodel = Ridge(alpha = 1) #optimal alpha = 1\nX_ft= add_polynomial_features(X, 10) #optimal features = 10\nmodel.fit(X_ft, y) #fit the model\ncoef_idx = list(X_ft.columns).index(\"race_d\") #getting index of the coefficents for race\nrace_coef = model.coef_[coef_idx] # pulling out specific coefficent\nprint(\"Black Coefficent: \", race_coef)\n\nrace_coef_exp = np.exp(race_coef) #getting %\nprint(\"Black percent of spend: \", race_coef_exp)\n\nBlack Coefficent:  -0.26704550927753734\nBlack percent of spend:  0.7656382270299403\n\n\nBlack people only spend 77% of what White people do for the same medical problems. This is in line with Obermeyer et al. (2019), as they presented the same research, Black people’s risk is weighted less than white people’s because of disparities in historic spending, coming from systemic problems."
  },
  {
    "objectID": "posts/blog-4/blog-4.html#discussion",
    "href": "posts/blog-4/blog-4.html#discussion",
    "title": "Blog 4- Replication Study",
    "section": "Discussion",
    "text": "Discussion\nFrom this analysis, we find that the racial disparity in risk scores stems back to racial disparity in medical costs, which come from systemic racism. This is why the risk score is a flawed and biased measure. The bias is best described by disparate impact, as discussed in Chapter 3 of Fairness and Machine Learning. The algorithm used healthcare costs as a proxy for health, but because Black patients historically receive less medical spending due to systemic racism, the model underestimated their health needs. This led to Black patients being less likely to receive the same level of care recommendations as white patients, even though they were equally sick. The study supports this by showing that at the same risk score, Black patients had worse health outcomes, proving that cost was a flawed proxy."
  },
  {
    "objectID": "posts/blog-6/warmup311.html",
    "href": "posts/blog-6/warmup311.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\nclass LinearModel:\n\n    def __init__(self):\n        self.w = None \n\n    def score(self, X):\n        \"\"\"\n        Compute the scores for each data point in the feature matrix X. \n        The formula for the ith entry of s is s[i] = &lt;self.w, x[i]&gt;. \n\n        If self.w currently has value None, then it is necessary to first initialize self.w to a random value. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            s torch.Tensor: vector of scores. s.size() = (n,)\n        \"\"\"\n        if self.w is None: \n            self.w = torch.rand((X.size()[1]))\n\n        return X @ self.w\n\n    def predict(self, X):\n        \"\"\"\n        Compute the predictions for each data point in the feature matrix X. The prediction for the ith data point is either 0 or 1. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            y_hat, torch.Tensor: vector predictions in {0.0, 1.0}. y_hat.size() = (n,)\n        \"\"\"\n        scores= self.score(X)\n        return (scores &gt; 0).float()\n\nclass Perceptron(LinearModel):\n\n    def loss(self, X, y):\n        \"\"\"\n        Compute the misclassification rate. A point i is classified correctly if it holds that s_i*y_i_ &gt; 0, where y_i_ is the *modified label* that has values in {-1, 1} (rather than {0, 1}). \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n            y, torch.Tensor: the target vector.  y.size() = (n,). The possible labels for y are {0, 1}\n        \n        HINT: In order to use the math formulas in the lecture, you are going to need to construct a modified set of targets and predictions that have entries in {-1, 1} -- otherwise none of the formulas will work right! An easy to to make this conversion is: \n        \n        y_ = 2*y - 1\n        \"\"\"\n        \n        y_ = 2 * y - 1 \n        scores = self.score(X)\n        misclassified = (scores * y_ &lt;= 0).float()\n        return misclassified.mean()\n\n    def grad(self, X, y):\n        pass \n\nclass PerceptronOptimizer:\n\n    def __init__(self, model):\n        self.model = model \n    \n    def step(self, X, y):\n        \"\"\"\n        Compute one step of the perceptron update using the feature matrix X \n        and target vector y. \n        \"\"\"\n        pass\n\n\np = Perceptron()\ns = p.score(X)\nl = p.loss(X, y)\nprint(l == 0.5)\n\ntensor(True)"
  },
  {
    "objectID": "posts/blog-7/blog-7.html",
    "href": "posts/blog-7/blog-7.html",
    "title": "Blog 6- Perceptron",
    "section": "",
    "text": "Logisitic Regression Code"
  },
  {
    "objectID": "posts/blog-7/blog-7.html#abstract",
    "href": "posts/blog-7/blog-7.html#abstract",
    "title": "Blog 6- Perceptron",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement logisitic regression and gradient descent and test in on a series of data, some simulated and some real world data. We find that logistic regression is a great method for binary classification, especially when we play around with the momentum term beta. increasing the momentum term makes the algorithim more efficent when it optimizes each step.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/blog-7/blog-7.html#expirements",
    "href": "posts/blog-7/blog-7.html#expirements",
    "title": "Blog 6- Perceptron",
    "section": "Expirements",
    "text": "Expirements\n\nData and plotting setup\n\nimport torch\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(n_points = 300, noise = 0.2 ,p_dims =2)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, w):\n    plt.figure(figsize=(4, 3))\n\n    colors = ['RED', 'BLUE']\n\n    # Scatter plot of the data points with consistent colors\n    for label in [0, 1]:\n        plt.scatter(X[y == label, 0], X[y == label, 1], \n                    color=colors[label], label=f'Class {label}', edgecolor='k', alpha=0.7)\n\n    # Create a grid to evaluate the model\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    # Ensure w is a NumPy array and flatten it\n    w = np.array(w).flatten()\n\n    # Predict over the grid\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    if len(w) == 3:  # Assuming w includes a bias term\n        preds = np.dot(grid, w[:-1]) + w[-1]\n    else:\n        preds = np.dot(grid, w)\n    preds = preds.reshape(xx.shape)\n\n    # Plot the shaded regions with simple colors\n    plt.contourf(xx, yy, preds &gt; 0, alpha=0.3, colors=['RED', 'BLUE'])\n\n    # Plot the decision boundary (line where prediction is zero)\n    plt.contour(xx, yy, preds, levels=[0], colors=\"black\", linewidths=2)\n\n    plt.title(\"Decision Boundary with Shaded Regions\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()\n\n\n# Plot the loss curve\ndef plot_loss(loss_vec):\n    plt.figure(figsize=(4, 3))\n    plt.plot(loss_vec, label=\"Loss\")\n    plt.title(\"Loss Curve\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n\n\nVanilla Gradient Descent\nThis first expirement tests the validity of the logisitic regression function by using simple data and no value of beta. This will help to demonstrate that the methods implemented work like they are supposed to\n\nimport torch\ntorch.manual_seed(1235)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nX, y = classification_data(n_points = 300, noise = 0.2 ,p_dims =2)\nloss_vec = []\n\n\n# Vanilla gradient descent\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n\n# Create figure for plotting decision boundary\n\n# Run gradient descent\nfor i in range(100):\n    loss = LR.loss(X, y)\n    loss_vec.append(loss.item())  # Store the loss for plotting\n    # Perform one step of gradient descent\n    opt.step(X, y, alpha=0.1, beta=0)\nw= LR.w\nprint(LR.w)\n\nplot_decision_boundary(X, y, w)\nplot_loss(loss_vec)\n\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\ntensor([ 8.3854,  8.4834, -8.0724])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we observe a weight vector that converges properly, dividing the data at a proper descision boundary. We also see clear monotonic loss decrease. All of this happens over 100 iterations of gradient descent (our optimizer), with a relativley low value of alpha and no momentum term (beta). This behavior is similar to what we saw in our perceptron algorithim.\n\n\nBenefits of Momentum\nIn this expierement, I use the same data as above but play around with the momentum term to see the benefits of adding it. The expected result is faster convergene in the loss function.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nX, y = classification_data(n_points = 300, noise = 0.2 ,p_dims =2)\n\n\n## Benefits of momentum\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\nloss_vec = []\n# Create figure for plotting decision boundary\n\n# Run gradient descent\nfor i in range(100):\n    loss = LR.loss(X, y)\n    loss_vec.append(loss.item())  # Store the loss for plotting\n    # Perform one step of gradient descent\n    opt.step(X, y, alpha=0.01, beta=.9)\nw= LR.w\nprint(LR.w)\n\nplot_decision_boundary(X, y, w)\nplot_loss(loss_vec)\n\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\nself.model.w torch.Size([3])\nprev torch.Size([3])\ngrad torch.Size([3])\ntorch.Size([3])\ntensor([ 12.2688,  12.6517, -12.0516])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we add the momentum term of beta = .9, we find that the loss converges much faster than it did in the vanilla gradient descent, and a more accurate line dividing the two classes. This is because the momentum accelerates the process of graident descent by magnifing the differences between the current and previous weights.\n\n\nOverfitting\nIn this section, I explore the harms of overfitting by creating data with fewer points than dimensions. Then I run the training loop to understand the effects of overtraining by comparing it to testing data. I expect to see lower accuracy on the testing data due to overfitting.\n\n## Overfitting data\nimport torch\ndef overfit_data(n_points = 50, noise = 0.5, p_dims = 100):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    y = 1.0 * y\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)  # Add bias term\n    \n    return X, y\n\nX_train, y_train = overfit_data(n_points = 10, noise = 0.3 ,p_dims =100)\n\nX_test, y_test = overfit_data(n_points = 10, noise = 0.3 ,p_dims =100)\n\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n# Create figure for plotting decision boundary\naccuracy= 0\n# Run gradient descent\nwhile accuracy &lt; 1:\n    loss = LR.loss(X_train, y_train)\n    loss_vec.append(loss.item())  # Store the loss for plotting\n    # Perform one step of gradient descent\n    opt.step(X_train, y_train, alpha=0.1, beta=.1)\n    y_pred = LR.predict(X_train)\n    accuracy = (y_pred == y_train).float().mean().item()\nw= LR.w\ny_pred = LR.predict(X_train)\n\n# Calculate accuracy\n\nprint(f'Training Accuracy: {accuracy * 100:.2f}%')\n\ny_pred_test = LR.predict(X_test)\n\n# Calculate accuracy on the testing data\naccuracy_test = (y_pred_test == y_test).float().mean().item()\nprint(f'Testing Accuracy: {accuracy_test * 100:.2f}%')\n\nself.model.w torch.Size([101])\nprev torch.Size([101])\ngrad torch.Size([101])\ntorch.Size([101])\nself.model.w torch.Size([101])\nprev torch.Size([101])\ngrad torch.Size([101])\ntorch.Size([101])\nself.model.w torch.Size([101])\nprev torch.Size([101])\ngrad torch.Size([101])\ntorch.Size([101])\nTraining Accuracy: 100.00%\nTesting Accuracy: 70.00%\n\n\nBy setting the number of dimensions the data trains on to more points than the data has, we expierence overfitting. We see this because although the data is structured in the exact same way, we get much lower accuracy on the testing data. This is because we are intentionally training the model to fit into the nooks and crannies of the data in the training set which don’t apply to the testing data\n\n\nReal World Data\nIn this section, I run my model on real world data. I use M Yasser H’s dataset, which is data of every person who was on the titanic, their class, gender and other defining charecteristics and weather they lived or died. That status is what I use as the binary classifier.\n\nimport pandas as pd\ndata= pd.read_csv(\"Titanic-Dataset.csv\")\n\ndata['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\nX = data[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare']]\ny = data['Survived']\nX = X.fillna(0)\ntarget = 'Survived'\nX.head()\n\n\n\n\n\n\n\n\nPclass\nSex\nSibSp\nParch\nFare\n\n\n\n\n0\n3\n0\n1\n0\n7.2500\n\n\n1\n1\n1\n1\n0\n71.2833\n\n\n2\n3\n1\n0\n0\n7.9250\n\n\n3\n1\n1\n1\n0\n53.1000\n\n\n4\n3\n0\n0\n0\n8.0500\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\ntorch.manual_seed(13)\n# Convert to tensors\nX = torch.tensor(X.values, dtype=torch.float32)\n#X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\ny = torch.tensor(y.values, dtype=torch.float32)\n\n# Flatten y to ensure it matches the simulated data\ny = y.flatten()\n\n#Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n\n# initialize model and optimizer\nLR = LogisticRegression()\n#LR.w = torch.zeros((X_train.shape[1], 1), requires_grad=True)\nopt = GradientDescentOptimizer(LR)\n\n#track loss for training and validation\nloss_vec = []\nval_loss_vec = []\n\nfor i in range(1000):\n    #calculate loss for training set\n    loss = LR.loss(X_train, y_train)\n    loss_vec.append(loss.item())  # Store the loss for plotting\n    #print(loss)\n    #calculate validation loss\n    val_loss = LR.loss(X_val, y_val)\n    val_loss_vec.append(val_loss.item())\n\n    #perform one step of gradient descent\n    opt.step(X_train, y_train, alpha=0.0000001, beta=0) \n\n\n# plot loss\nprint(\"Momentum:\")\nplot_loss(loss_vec)  # Training loss curve\nplot_loss(val_loss_vec)  # Validation loss curve\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n#track loss for training and validation\nloss_vec = []\nval_loss_vec = []\n\n# Run gradient descent\nfor i in range(1000):\n    #calculate loss for training set\n    loss = LR.loss(X_train, y_train)\n    loss_vec.append(loss.item())  # Store the loss for plotting\n    #print(loss)\n    #calculate validation loss\n    val_loss = LR.loss(X_val, y_val)\n    val_loss_vec.append(val_loss.item())\n\n    #perform one step of gradient descent\n    opt.step(X_train, y_train, alpha=0.000001, beta=.9) \n\n# plot loss\nprint(\"No Momentum:\")\nplot_loss(loss_vec)  # Training loss curve\nplot_loss(val_loss_vec)  # Validation loss curve\n\ny_test_pred = LR.predict(X_test)\ntest_accuracy = (y_test_pred == y_test).float().mean().item()\n\n# # Test accuracy\n# y_test_pred = LR.predict(X_test)\n# test_accuracy = (y_test_pred == y_test).float().mean().item()\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\nprint(f'Testing Loss: {LR.loss(X_test, y_test):.2f}')\n\nMomentum:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo Momentum:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Accuracy: 72.07%\nTesting Loss: 0.55\n\n\nI was able to get a test accuracy of 72%, which is pretty solid. This means that in 72% of testing cases, the model correctly predicted weather someone on the ship lived or died based on factors like their class, their fare and their gender. I also run two different iterations, one that has momentum (the first one), and one with no momentum (the second one). We can see clearly that the equation with the momentum converges more quickly to a loss value over the same time period. We also see lower loss over the whole period for the non-momentum term."
  },
  {
    "objectID": "posts/blog-7/blog-7.html#discussion",
    "href": "posts/blog-7/blog-7.html#discussion",
    "title": "Blog 6- Perceptron",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I write and implement logisitic regression and graident descent. To test out these functions, I compute I plot the descision regions and comupte the scores and losses on different sets of data. We find that logisitic regression is optimal for doing binary classification tasks, even on real world data."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 6- Perceptron\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nMar 19, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 6- Perceptron\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nMar 19, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4- Replication Study\n\n\n\n\n\nReplicating Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\n\n\n\n\n\nMar 10, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3- Auditing Bias\n\n\n\n\n\nAuditing Employment Bias\n\n\n\n\n\nMar 8, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Two - Design and Impact of Automated Decision Systems\n\n\n\n\n\nBank Loaning\n\n\n\n\n\nFeb 27, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog One - Palmer Penguins\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nFeb 16, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]