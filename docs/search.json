[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/blog-1/blog_1.html",
    "href": "posts/blog-1/blog_1.html",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "image.png\n\n\n\n\nIn my blog post, I explore the best features to identify penguin species from the Palmer Penguins data set created by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Dr. Kristen Gorman ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Palmer Station. I first explore the significance of some variables through data visualization, trying to glean key differences in penguin species, one of which appears to be island. Using a brute force search of all the possible combinations of one qualitative and two quantitative variables, I find that island, culmen length, and culmen depth are the three best features to predict the test set with 100% accuracy.\n\n\n\n\nimport pandas as pd\n\n# reading in the training data\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url) \n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can see from the head of the training data some of the key variables to look at: island, clutch completion, culmen length/depth, flipper length and body mass. These variables jump out as the oens where we will likely see the biggest differences between species.\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #encoding species\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #dropping uncessary variables\n  df = df[df[\"Sex\"] != \".\"] \n  df = df.dropna() #cleaning data\n  y = le.transform(df[\"Species\"]) #encoding species\n  df = df.drop([\"Species\"], axis = 1) \n  df = pd.get_dummies(df) \n  return df, y\n\n#creating test and train datasets\nX_train, y_train = prepare_data(train) \n\nIn the data preperation section, I prepare the data by dropping the variables that aren’t helpful for the analysis. I also use the label encoder function to encode the target variable (species). I also clean the data so I can prepare the features and the labels for training.\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n## graph1 scatter + weight distribtuion\n\ndata= train\ngraph1 = sns.jointplot(data= data, x= \"Flipper Length (mm)\", y= \"Body Mass (g)\", hue = \"Species\")\nplt.show\n\n\n\n\n\n\n\n\nThe main takeaway I had from this graph is that flipper length is a great identifier for the Gentoo penguins, as they all have flipper lengths at about 208 mm or above, which is much longer than the average of an Adelie or a Chinstrap penguin. However, the body mass and flipper length are similar for and Chinstrap penguins, which means that they might not be the best identifier of a penguin. We can see that the distributions for body mass of the two species have almost complete overlap, with Adelie penguins having less devation in mass by penguin.\n\n# graph 2: catplot\ndata = train\ngraph2= sns.catplot(data= data, kind= \"swarm\", x= \"Island\", y=\"Culmen Length (mm)\", hue= \"Species\")\n\n\n\n\n\n\n\n\nThis visualization is very helpful for deciding which categories to model. The first thing I notice is that the Chinstrap and Gentoo species are native to one island and one island only (dream and biscoe, respectiveley). While the Adelie penguins appear on all 3 islands, they are the only ones that appear on Torgenrsen, so any penguin found on Torgersen should automatically be identified as a Adelie. Culmen length is also a good diffentiator,\n\ndata.groupby(\"Species\").aggregate({\"Flipper Length (mm)\" : \"mean\", \n                                   \"Culmen Length (mm)\" : \"mean\",\n                                    \"Culmen Depth (mm)\" : \"mean\",\n                                    \"Body Mass (g)\" : [\"mean\", \"min\", \"max\"]})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nmean\nmean\nmean\nmin\nmax\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n3718.487395\n2850.0\n4725.0\n\n\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n3743.421053\n2700.0\n4800.0\n\n\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n5039.948454\n3950.0\n6300.0\n\n\n\n\n\n\n\nThis table shows clear distinctions between the penguins for the different quantitative variables highlighted here. For example, while Adelie and Chinstraps have similar flipper lengths, masses, and culmen depths, they vary vastly in clumen length. Gentoos can be differentiated through their much larger average body mass and longer flippers. There shouild be very little overlap betrween Adelie and Gentoo, as they very significantly on the different quantitative variables.\n\n\n\n\n%%capture\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Stage\", \"Sex\"] \nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\" ]\n\nbest_score= 0\nfor qual in all_qual_cols: #iterating through all possible combinations of columns\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2): \n    cols = list(pair) + qual_cols\n    #print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train) #fitting the model\n    score = LR.score(X_train[cols], y_train) #scoring the model\n    if score &gt; best_score: #saving the best score\n      best_score = score\n      best_cols = cols\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\nThe way I approach selecting the best features is to use a nested loop to run through all possible combinations of one qualitative and two quantitative variables. I keep track of the training model with the best score and save those features.\n\n%%capture\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train) #running the model on the best features\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nHere I run the training model on my best selected features and load in the test data.\n\n#preparing the test set and testing it\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nI calculate the score here for the model established on the testing data and get an accuracy score of 100%.\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nprint(best_cols)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# plotting the decision reigon plot\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(LR, X_train[best_cols], y_train)\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the decision plots for the training and the testing data sets. As we saw in the explore section, certain species are native to certain islands. In the trainig data, we see that some Chinstrap penguins are on the boundary between Chinstrap and Gentoo reigons (causing slight misclassification), but due to the subset of data in the testing set, none of these penguins are super close to the boarder, so everything is classified correctly.\n\nfrom sklearn.metrics import confusion_matrix\n\n# creating confusion matricies for the test and train datasets.\ny_test_pred = LR.predict(X_test[best_cols])\nConfusion_test = confusion_matrix(y_test, y_test_pred)\nprint(Confusion_test) \ny_train_pred = LR.predict(X_train[best_cols])\nConfusion_train = confusion_matrix(y_train, y_train_pred)\nprint(Confusion_train)\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n[[108   0   0]\n [  1  55   0]\n [  0   0  92]]\n\n\nSince there are no errors in the test set, the first confusion matrix is diagonalizable. There are no penguins that are classified incorrectly. Because of this, I also created the same matrix for the train dataset, where one Chinstrap penguin was classified an Adelie. I wonder if this could be in part because the number of penguins in each set where drastically different, and since there a lot more Adelie I wonder if penguins were more likely to be classified as such.\n\n\n\nMy key takeaway from the analysis is that when using one feature on it’s own, or a pair of correlating features only (i.e. culmen length and culmen depth), logistic regression is decently accurate. The more features you pair together, the more accurate the logistic regression becomes, although the computing power escalates quickly. Island is a great quantitative identifier for penguins, as shown in figure 2 because Chinstraps and Gentoos exist only on different islands, so there is no overlap or mispredicting.\nOn competing my first blog post, I found the process very rewarding. I like being able to write out my ideas and complete my analysis all in the same document. I haven’t coded in a while, so debugging at first was difficult, but it got easier as I went on. I learned that, with both the coding and the discussion, it’s often best to step away for a bit and come back to the work with fresh eyes. That helped me when I was looking for significance in the explore step, and throughout debugging my model and the ensuing discussion."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#abstract",
    "href": "posts/blog-1/blog_1.html#abstract",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "In my blog post, I explore the best features to identify penguin species from the Palmer Penguins data set created by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Dr. Kristen Gorman ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Palmer Station. I first explore the significance of some variables through data visualization, trying to glean key differences in penguin species, one of which appears to be island. Using a brute force search of all the possible combinations of one qualitative and two quantitative variables, I find that island, culmen length, and culmen depth are the three best features to predict the test set with 100% accuracy."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#setup",
    "href": "posts/blog-1/blog_1.html#setup",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "import pandas as pd\n\n# reading in the training data\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url) \n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can see from the head of the training data some of the key variables to look at: island, clutch completion, culmen length/depth, flipper length and body mass. These variables jump out as the oens where we will likely see the biggest differences between species."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#data-preperation",
    "href": "posts/blog-1/blog_1.html#data-preperation",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #encoding species\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #dropping uncessary variables\n  df = df[df[\"Sex\"] != \".\"] \n  df = df.dropna() #cleaning data\n  y = le.transform(df[\"Species\"]) #encoding species\n  df = df.drop([\"Species\"], axis = 1) \n  df = pd.get_dummies(df) \n  return df, y\n\n#creating test and train datasets\nX_train, y_train = prepare_data(train) \n\nIn the data preperation section, I prepare the data by dropping the variables that aren’t helpful for the analysis. I also use the label encoder function to encode the target variable (species). I also clean the data so I can prepare the features and the labels for training."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#explore",
    "href": "posts/blog-1/blog_1.html#explore",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n## graph1 scatter + weight distribtuion\n\ndata= train\ngraph1 = sns.jointplot(data= data, x= \"Flipper Length (mm)\", y= \"Body Mass (g)\", hue = \"Species\")\nplt.show\n\n\n\n\n\n\n\n\nThe main takeaway I had from this graph is that flipper length is a great identifier for the Gentoo penguins, as they all have flipper lengths at about 208 mm or above, which is much longer than the average of an Adelie or a Chinstrap penguin. However, the body mass and flipper length are similar for and Chinstrap penguins, which means that they might not be the best identifier of a penguin. We can see that the distributions for body mass of the two species have almost complete overlap, with Adelie penguins having less devation in mass by penguin.\n\n# graph 2: catplot\ndata = train\ngraph2= sns.catplot(data= data, kind= \"swarm\", x= \"Island\", y=\"Culmen Length (mm)\", hue= \"Species\")\n\n\n\n\n\n\n\n\nThis visualization is very helpful for deciding which categories to model. The first thing I notice is that the Chinstrap and Gentoo species are native to one island and one island only (dream and biscoe, respectiveley). While the Adelie penguins appear on all 3 islands, they are the only ones that appear on Torgenrsen, so any penguin found on Torgersen should automatically be identified as a Adelie. Culmen length is also a good diffentiator,\n\ndata.groupby(\"Species\").aggregate({\"Flipper Length (mm)\" : \"mean\", \n                                   \"Culmen Length (mm)\" : \"mean\",\n                                    \"Culmen Depth (mm)\" : \"mean\",\n                                    \"Body Mass (g)\" : [\"mean\", \"min\", \"max\"]})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nmean\nmean\nmean\nmin\nmax\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n3718.487395\n2850.0\n4725.0\n\n\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n3743.421053\n2700.0\n4800.0\n\n\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n5039.948454\n3950.0\n6300.0\n\n\n\n\n\n\n\nThis table shows clear distinctions between the penguins for the different quantitative variables highlighted here. For example, while Adelie and Chinstraps have similar flipper lengths, masses, and culmen depths, they vary vastly in clumen length. Gentoos can be differentiated through their much larger average body mass and longer flippers. There shouild be very little overlap betrween Adelie and Gentoo, as they very significantly on the different quantitative variables."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#model",
    "href": "posts/blog-1/blog_1.html#model",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "%%capture\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Stage\", \"Sex\"] \nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\" ]\n\nbest_score= 0\nfor qual in all_qual_cols: #iterating through all possible combinations of columns\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2): \n    cols = list(pair) + qual_cols\n    #print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train) #fitting the model\n    score = LR.score(X_train[cols], y_train) #scoring the model\n    if score &gt; best_score: #saving the best score\n      best_score = score\n      best_cols = cols\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\nThe way I approach selecting the best features is to use a nested loop to run through all possible combinations of one qualitative and two quantitative variables. I keep track of the training model with the best score and save those features.\n\n%%capture\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train) #running the model on the best features\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nHere I run the training model on my best selected features and load in the test data.\n\n#preparing the test set and testing it\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nI calculate the score here for the model established on the testing data and get an accuracy score of 100%."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#evaluate",
    "href": "posts/blog-1/blog_1.html#evaluate",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nprint(best_cols)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# plotting the decision reigon plot\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(LR, X_train[best_cols], y_train)\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the decision plots for the training and the testing data sets. As we saw in the explore section, certain species are native to certain islands. In the trainig data, we see that some Chinstrap penguins are on the boundary between Chinstrap and Gentoo reigons (causing slight misclassification), but due to the subset of data in the testing set, none of these penguins are super close to the boarder, so everything is classified correctly.\n\nfrom sklearn.metrics import confusion_matrix\n\n# creating confusion matricies for the test and train datasets.\ny_test_pred = LR.predict(X_test[best_cols])\nConfusion_test = confusion_matrix(y_test, y_test_pred)\nprint(Confusion_test) \ny_train_pred = LR.predict(X_train[best_cols])\nConfusion_train = confusion_matrix(y_train, y_train_pred)\nprint(Confusion_train)\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n[[108   0   0]\n [  1  55   0]\n [  0   0  92]]\n\n\nSince there are no errors in the test set, the first confusion matrix is diagonalizable. There are no penguins that are classified incorrectly. Because of this, I also created the same matrix for the train dataset, where one Chinstrap penguin was classified an Adelie. I wonder if this could be in part because the number of penguins in each set where drastically different, and since there a lot more Adelie I wonder if penguins were more likely to be classified as such."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#discussion",
    "href": "posts/blog-1/blog_1.html#discussion",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "My key takeaway from the analysis is that when using one feature on it’s own, or a pair of correlating features only (i.e. culmen length and culmen depth), logistic regression is decently accurate. The more features you pair together, the more accurate the logistic regression becomes, although the computing power escalates quickly. Island is a great quantitative identifier for penguins, as shown in figure 2 because Chinstraps and Gentoos exist only on different islands, so there is no overlap or mispredicting.\nOn competing my first blog post, I found the process very rewarding. I like being able to write out my ideas and complete my analysis all in the same document. I haven’t coded in a while, so debugging at first was difficult, but it got easier as I went on. I learned that, with both the coding and the discussion, it’s often best to step away for a bit and come back to the work with fresh eyes. That helped me when I was looking for significance in the explore step, and throughout debugging my model and the ensuing discussion."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog One - Palmer Penguins\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nFeb 16, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]