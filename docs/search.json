[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/blog-3/blog_3.html",
    "href": "posts/blog-3/blog_3.html",
    "title": "Blog 3- Auditing Bias",
    "section": "",
    "text": "In this blog post, I look at racial employment bias in Ohio through ACS data. I do this by creating a model using an optimized descision tree. From this model, I do an audit to detect racism, although race was not an attribute the data was trained on. I find that there is some racisim in the model, as their is lack of error rate balance for Black people, bi-racial people, Asian people and Indegenous people. I then analyze my findings in the broader context of employers using this algorithim in the hiring process, concluding that the use of this algorithim may prepetuate systemic racism, and it is to not be used unless in conjunction with diverse hiring practices."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#abstract",
    "href": "posts/blog-3/blog_3.html#abstract",
    "title": "Blog 3- Auditing Bias",
    "section": "",
    "text": "In this blog post, I look at racial employment bias in Ohio through ACS data. I do this by creating a model using an optimized descision tree. From this model, I do an audit to detect racism, although race was not an attribute the data was trained on. I find that there is some racisim in the model, as their is lack of error rate balance for Black people, bi-racial people, Asian people and Indegenous people. I then analyze my findings in the broader context of employers using this algorithim in the hiring process, concluding that the use of this algorithim may prepetuate systemic racism, and it is to not be used unless in conjunction with diverse hiring practices."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#setup-for-example",
    "href": "posts/blog-3/blog_3.html#setup-for-example",
    "title": "Blog 3- Auditing Bias",
    "section": "Setup for Example",
    "text": "Setup for Example\n\n#loading and setting up data\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns\n\n\n\n\n#selecting recomended possible features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n60\n15.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n1\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n54\n18.0\n3\n16\n1\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n20\n18.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n33\n18.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n6.0"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#example",
    "href": "posts/blog-3/blog_3.html#example",
    "title": "Blog 3- Auditing Bias",
    "section": "Example",
    "text": "Example\n\n# Setting up the employment problem\nimport warnings\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data) \n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(99419, 15)\n(99419,)\n(99419,)\n\n\n\n#create train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n#creating model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression()) #scales and performs a LR for the model\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\ny_hat = model.predict(X_test)\nprint((y_hat == y_test).mean())\nprint((y_hat == y_test)[group_test == 1].mean())\nprint((y_hat == y_test)[group_test == 2].mean())\n\n0.7863608931804466\n0.7875706214689265\n0.7777164920022063"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#setup",
    "href": "posts/blog-3/blog_3.html#setup",
    "title": "Blog 3- Auditing Bias",
    "section": "Setup",
    "text": "Setup\n\n#loading and setting up data\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"OH\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000037\n3\n1\n908\n2\n39\n1013097\n12\n82\n...\n10\n12\n0\n11\n20\n22\n22\n12\n10\n20\n\n\n1\nP\n2018GQ0000068\n3\n1\n4200\n2\n39\n1013097\n75\n45\n...\n76\n17\n77\n77\n78\n78\n15\n80\n77\n78\n\n\n2\nP\n2018GQ0000126\n3\n1\n3500\n2\n39\n1013097\n34\n50\n...\n33\n34\n3\n36\n35\n36\n34\n4\n35\n34\n\n\n3\nP\n2018GQ0000156\n3\n1\n400\n2\n39\n1013097\n53\n19\n...\n53\n53\n94\n55\n100\n96\n9\n7\n53\n50\n\n\n4\nP\n2018GQ0000168\n3\n1\n4700\n2\n39\n1013097\n14\n18\n...\n27\n14\n27\n14\n15\n27\n15\n26\n2\n15\n\n\n\n\n5 rows × 286 columns\n\n\n\n\n#selecting recomended possible features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n82\n12.0\n2\n16\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n2\n2\n6.0\n\n\n1\n45\n18.0\n5\n16\n1\nNaN\n2\n3.0\n4.0\n1\n1\n1\n2\n2.0\n1\n1\n6.0\n\n\n2\n50\n14.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n3\n19\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n1\n3.0\n\n\n4\n18\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n1.0"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#model",
    "href": "posts/blog-3/blog_3.html#model",
    "title": "Blog 3- Auditing Bias",
    "section": "Model",
    "text": "Model\n\n# Setting up the employment problem\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data) \n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]] #taking out employment status (target) and race (so we can look at bias)\n\n(119086, 15)\n(119086,)\n(119086,)\n\n\n\n#train test split\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier', DecisionTreeClassifier())]) StandardScaler?Documentation for StandardScalerStandardScaler() DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n'''\nIn this chunk, I perform a grid search to find the optimal maxmimum depth for my decsision tree model. \nI calculate the best model based on the best maxmimum depth\n'''\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"decisiontreeclassifier__max_depth\":[3, 5, 10, 15] #Testing different depths\n}\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\")  #performing grid search\ngrid_search.fit(X_train, y_train) #training model for each hyper parameter\n\nprint(\"Best maximum depth:\", grid_search.best_params_[\"decisiontreeclassifier__max_depth\"])\nprint(\"Best score:\", grid_search.best_score_)\nbest_model = grid_search.best_estimator_\n\nBest maximum depth: 10\nBest score: 0.8342780636916801"
  },
  {
    "objectID": "posts/blog-3/blog_3.html#data-reading-visualization",
    "href": "posts/blog-3/blog_3.html#data-reading-visualization",
    "title": "Blog 3- Auditing Bias",
    "section": "Data Reading / Visualization",
    "text": "Data Reading / Visualization\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf[\"ESR\"] = (df[\"label\"] == True).astype(int) #creating numeric values for employment status\ndf[\"gender\"]= df[\"SEX\"].map({1: \"Male\", 2: \"Female\"}) #creating categorical labels for gender\n\n\n#Making race into a categorical variable\nrace_map={\n    1 : \"White\",\n    2 : \"Black\",\n    3 : \"American Indigenous\",\n    4 : \"Alaskan Native\",\n    5 : \"American Indigenous, Alaskan Native tribe specified\",\n    6 : \"Asian\",\n    7 : \"Native Hawaiian or Pacific Islander\",\n    8 : \"Some other race\",\n    9 : \"Two or more races\"\n}\ndf[\"race\"] = df[\"group\"].map(race_map) #turning race into a categorical variable\n\n\ntotal_people = len(df)\nproportion_of_people_employed= df[\"ESR\"].mean()\nemployed_by_race= df.groupby([\"race\", \"gender\"])[\"ESR\"].mean().reset_index()\npeople_per_race = df[\"race\"].value_counts()\n\nprint(total_people)\nprint(proportion_of_people_employed)\nprint(people_per_race)\n\n95268\n0.46178150060880885\nrace\nWhite                                                  81427\nBlack                                                   8765\nTwo or more races                                       2399\nAsian                                                   1817\nSome other race                                          654\nAmerican Indigenous                                      119\nAmerican Indigenous, Alaskan Native tribe specified       62\nNative Hawaiian or Pacific Islander                       24\nAlaskan Native                                             1\nName: count, dtype: int64\n\n\n\nemployed_by_race\n\n\n\n\n\n\n\n\nrace\ngender\nESR\n\n\n\n\n0\nAlaskan Native\nFemale\n0.000000\n\n\n1\nAmerican Indigenous\nFemale\n0.447761\n\n\n2\nAmerican Indigenous\nMale\n0.461538\n\n\n3\nAmerican Indigenous, Alaskan Native tribe spec...\nFemale\n0.260870\n\n\n4\nAmerican Indigenous, Alaskan Native tribe spec...\nMale\n0.307692\n\n\n5\nAsian\nFemale\n0.460300\n\n\n6\nAsian\nMale\n0.568362\n\n\n7\nBlack\nFemale\n0.421274\n\n\n8\nBlack\nMale\n0.371907\n\n\n9\nNative Hawaiian or Pacific Islander\nFemale\n0.583333\n\n\n10\nNative Hawaiian or Pacific Islander\nMale\n0.583333\n\n\n11\nSome other race\nFemale\n0.433657\n\n\n12\nSome other race\nMale\n0.449275\n\n\n13\nTwo or more races\nFemale\n0.287490\n\n\n14\nTwo or more races\nMale\n0.300336\n\n\n15\nWhite\nFemale\n0.441879\n\n\n16\nWhite\nMale\n0.504908\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.barplot(data= employed_by_race, x=\"race\", y=\"ESR\", hue=\"gender\", palette={\"Male\": \"blue\", \"Female\": \"pink\"})\nplt.xticks(rotation=45);\n\n\n\n\n\n\n\n\nHow many individuals are in the data?\n-95268 Individuals are in the data\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\n-46.17% of people are employed\nOf these individuals, how many are in each of the groups?\n-We see that the largest group is white people, followed by Black people, bi-racial people, asians, other races and then indegenous people\nIn each group, what proportion of individuals have target label equal to 1?\n-See chart “employed_by_race”, of the categories with meaningful sample sizes (n &gt; 100), we see that Asian people have the highest employment rate (51%), followed by white people (47%), and the lowest rates of employment by race are bi-racial people and American indegenous.\nCheck for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex. This might be a good opportunity to use a visualization such as a bar chart, e.g. via the seaborn package.\n-Looking at employment by race, we can see that on average, men are more employed than women. The only racial group where women are more employed is Black people, where 42% of women are employed and 37% of men."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#model-audit",
    "href": "posts/blog-3/blog_3.html#model-audit",
    "title": "Blog 3- Auditing Bias",
    "section": "Model Audit",
    "text": "Model Audit\n\nOverall Audit\n\n#creating a testing dataframe\ndf_test = pd.DataFrame(X_test, columns = features_to_use) #pulling in the testing data\ny_hat = best_model.predict(X_test)\ndf_test[\"group\"] = group_test #adding back in race\ndf_test[\"label\"] = y_test #adding in employment status\ndf_test[\"ESR\"] = y_test.astype(int)\ndf_test[\"y_hat\"] = y_hat.astype(int) #adding in y_hat\ndf_test[\"race\"] = df_test[\"group\"].map(race_map) #making the race variable categorical\ndf_test.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\nESR\ny_hat\nrace\n\n\n\n\n0\n41.0\n16.0\n1.0\n1.0\n2.0\n0.0\n4.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n1\n1\nWhite\n\n\n1\n43.0\n21.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n0\n1\nWhite\n\n\n2\n47.0\n22.0\n5.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n1\n1\nWhite\n\n\n3\n78.0\n16.0\n2.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n0\n0\nWhite\n\n\n4\n13.0\n10.0\n5.0\n2.0\n2.0\n1.0\n1.0\n1.0\n0.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n0\n0\nWhite\n\n\n\n\n\n\n\n\n#Overall Accuracy\nprint(\"Accuracy: \", (y_hat == y_test).mean())\n\nAccuracy:  0.8319338315559661\n\n\n\n#Positive Predictive Value\nTP = np.sum((y_hat == 1) & (y_test == 1)) \nFP = np.sum((y_hat == 1) & (y_test == 0))\nTN = np.sum((y_hat == 0) & (y_test == 0))\nFN = np.sum((y_hat == 0) & (y_test == 1))\n\nPPV = TP / (TP + FP) #PPV formula\nprint(\"Positive Predictive Value: \", PPV)\n\nPositive Predictive Value:  0.8022638223770135\n\n\n\n#FPR and FNR rates\nFPR = FP / (FP + TN) \nFNR = FN / (FP + TP)\nprint(\"False Positive Rate: \", FPR)\nprint(\"False Negative Rate: \", FNR)\n\nFalse Positive Rate:  0.1764294592914854\nFalse Negative Rate:  0.15080539834566825\n\n\nWe see that the model has an overal accuracy of 83%, and a lower positive predictive value of 80%. We also see that the false negative rate (15%) is lower than the false positive rate (17%), so the model is more likely to predict someone is employed when they aren’t than prediciting someone is employed when they are.\n\n\nSubgroup accuracy\n\n#Accuracy on each sub group\ndf_test[\"correct_prediction\"] = (df_test[\"ESR\"] == df_test[\"y_hat\"]).astype(int) #sorting correct predictions as a column\nsubgroup_acc= df_test.groupby(\"race\")[\"correct_prediction\"].mean().reset_index() #sorting by race\nsubgroup_acc\n\n\n\n\n\n\n\n\nrace\ncorrect_prediction\n\n\n\n\n0\nAmerican Indigenous\n0.878788\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n0.866667\n\n\n2\nAsian\n0.806100\n\n\n3\nBlack\n0.813233\n\n\n4\nNative Hawaiian or Pacific Islander\n1.000000\n\n\n5\nSome other race\n0.856250\n\n\n6\nTwo or more races\n0.885099\n\n\n7\nWhite\n0.832698\n\n\n\n\n\n\n\n\n#PPV on each subgroup\nwarnings.simplefilter(\"ignore\")\n\ndef compute_ppv(group):\n    '''\n    computes the ppv of each group\n    args: group- race\n    returns: the ppv by subgroup\n    '''\n    tp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 1)).sum()\n    fp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 0)).sum()\n    ppv = tp / (tp + fp)\n    return pd.Series({\"PPV\": ppv})\n\n\nsubgroup_ppv = df_test.groupby(\"race\").apply(compute_ppv).reset_index() #applying ppv function by subgroup\n\nsubgroup_ppv\n\n\n\n\n\n\n\n\nrace\nPPV\n\n\n\n\n0\nAmerican Indigenous\n0.833333\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n1.000000\n\n\n2\nAsian\n0.783270\n\n\n3\nBlack\n0.741622\n\n\n4\nNative Hawaiian or Pacific Islander\n1.000000\n\n\n5\nSome other race\n0.775000\n\n\n6\nTwo or more races\n0.775862\n\n\n7\nWhite\n0.808863\n\n\n\n\n\n\n\n\n#FPR and FNR by subgroup\nwarnings.simplefilter(\"ignore\")\ndef calculate_fpr_fnr(group):\n    '''\n    Calcuates the false posititve and negative rates by group\n    Args: group- in our case rae\n    returns: the false positive and negative rates for each group\n    '''\n    fp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 0)).sum() #false positive calc\n    fn = ((group[\"y_hat\"] == 0) & (group[\"ESR\"] == 1)).sum() #false negative calc\n    tp = ((group[\"y_hat\"] == 1) & (group[\"ESR\"] == 1)).sum() #true positive calc\n    tn = ((group[\"y_hat\"] == 0) & (group[\"ESR\"] == 0)).sum() #true negative calc\n\n    fpr = fp / (fp + tn); #fpr rate\n    fnr = fn / (fn + tp); #fnr rate\n\n    return pd.Series({\"FPR\": fpr, \"FNR\": fnr})\n\nsubgroup_fpr_fnr = df_test.groupby(\"race\").apply(calculate_fpr_fnr).reset_index() #applying function\n\nsubgroup_fpr_fnr\n\n\n\n\n\n\n\n\nrace\nFPR\nFNR\n\n\n\n\n0\nAmerican Indigenous\n0.176471\n0.062500\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n0.000000\n0.400000\n\n\n2\nAsian\n0.257919\n0.134454\n\n\n3\nBlack\n0.185848\n0.188166\n\n\n4\nNative Hawaiian or Pacific Islander\nNaN\n0.000000\n\n\n5\nSome other race\n0.193548\n0.074627\n\n\n6\nTwo or more races\n0.098237\n0.156250\n\n\n7\nWhite\n0.176530\n0.156887\n\n\n\n\n\n\n\nWe see that the model performs decently well across different catgeories of race, especially for races with high numbers of observations. The accuracy for each subgroup is in the low to mid 80%s, which is pretty standard across races and matches the overall. The PPV is lowest for Black people (74%) and much higher for white people (80%), which is concerning, because it means white people overall are being predicted to be employed at higher rates. We also see pretty standard error rate balance across racial groups with large numbers of observations except for Asian and Indigenous people, where the rate of a false positive is much higher than the rate of a false negative.\n\n\nBias Measures\n\n#Is your model approximately calibrated?\ncalibration = df_test.groupby([\"y_hat\", \"race\"])[\"ESR\"].mean().unstack() #grouping y_hat by race and ESR to compare across races for values of y_hat\ncalibration\n\n\n\n\n\n\n\nrace\nAmerican Indigenous\nAmerican Indigenous, Alaskan Native tribe specified\nAsian\nBlack\nNative Hawaiian or Pacific Islander\nSome other race\nTwo or more races\nWhite\n\n\ny_hat\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.066667\n0.166667\n0.163265\n0.131841\nNaN\n0.0625\n0.065274\n0.144431\n\n\n1\n0.833333\n1.000000\n0.783270\n0.741622\n1.0\n0.7750\n0.775862\n0.808863\n\n\n\n\n\n\n\n\n#Does your model satisfy approximate error rate balance?\nwarnings.simplefilter(\"ignore\")\nerror_rates_by_race = df_test.groupby(\"race\").apply(calculate_fpr_fnr).reset_index() #using the FPR and NPR calculation from earlier\nerror_rates_by_race\n\n\n\n\n\n\n\n\nrace\nFPR\nFNR\n\n\n\n\n0\nAmerican Indigenous\n0.176471\n0.062500\n\n\n1\nAmerican Indigenous, Alaskan Native tribe spec...\n0.000000\n0.400000\n\n\n2\nAsian\n0.257919\n0.134454\n\n\n3\nBlack\n0.185848\n0.188166\n\n\n4\nNative Hawaiian or Pacific Islander\nNaN\n0.000000\n\n\n5\nSome other race\n0.193548\n0.074627\n\n\n6\nTwo or more races\n0.098237\n0.156250\n\n\n7\nWhite\n0.176530\n0.156887\n\n\n\n\n\n\n\n\n#Does your model satisfy statistical parity?\n\nsHR = 0.5 #threshold\ndf_test[\"score\"] = best_model.predict_proba(X_test)[:, 1] #creating score column\n\ndef compute_high_risk_rate(group):\n    '''\n    Computes a group's rate of unenmployment\n    args: group, in our case, race\n    returns: high_risk/total, the statistical parity for each group\n    '''\n    high_risk = (group[\"score\"] &lt; sHR).sum() #a person is \"high risk\" if the score is higher than the threshold\n    total = len(group)\n    return high_risk / total #creating an average\n\nstat_parity_by_race = df_test.groupby(\"race\").apply(compute_high_risk_rate).reset_index()\nprint(stat_parity_by_race)\n\n                                                race         0\n0                                American Indigenous  0.454545\n1  American Indigenous, Alaskan Native tribe spec...  0.800000\n2                                              Asian  0.427015\n3                                              Black  0.565462\n4                Native Hawaiian or Pacific Islander  0.000000\n5                                    Some other race  0.500000\n6                                  Two or more races  0.687612\n7                                              White  0.509482\n\n\nIn terms of bias measures, we see that the model is approximatley calibrated, with similar means of ESR for predicted values of ESR by race, espeically after taking out the groups that appeared very briefly in the survey data. We find that error rate parity is low for Asians, Indigenous people (higher false positives than false negatives) and bi-racial people (higher false negatives than false positives), but is pretty standard across other racial groups.The model also does a decent job of satisfying statistical parity, with a score of about .5 for races with high numbers of observations. The score however is higher for Black people and bi-racial people, and is lower for white people and Asians, meaning that they are less likely to be predicted as “high risk”.\n\n\nRe-creating the plot\n\ndf_test[\"FP\"] = ((df_test[\"y_hat\"] == 1) & (df_test[\"ESR\"] == 0)).astype(int)\ndf_test[\"FN\"] = ((df_test[\"y_hat\"] == 0) & (df_test[\"ESR\"] == 1)).astype(int)\ndf_test[\"TP\"] = ((df_test[\"y_hat\"] == 1) & (df_test[\"ESR\"] == 1)).astype(int)\ndf_test[\"TN\"] = ((df_test[\"y_hat\"] == 0) & (df_test[\"ESR\"] == 0)).astype(int)\ndf_test[\"FPR\"] = df_test[\"FP\"] / (df_test[\"FP\"] + df_test[\"TN\"]) #False Positive Rate\ndf_test[\"FNR\"] = df_test[\"FN\"] / (df_test[\"FN\"] + df_test[\"TP\"]) # False Negative Rate\nfpr_fnr_by_race = df_test.groupby(\"race\")[[\"FPR\", \"FNR\"]].mean().reset_index()\n\n\nwarnings.simplefilter(\"ignore\")\npalette = sns.color_palette(\"Set2\", n_colors=len(fpr_fnr_by_race))\n\ng = sns.lmplot(x=\"FP\", y=\"FN\", hue=\"race\", data=df_test, # Creating an lm plot using seaborn\n                y_jitter=.02, truncate=False)\ng.set(xlim=(0, 1), ylim=(0, .2))\n\nfor i, row in fpr_fnr_by_race.iterrows():\n    race_color = palette[i]  #Use the color corresponding to each race\n    plt.scatter(x=row[\"FPR\"], y=row[\"FNR\"], s=50, color=race_color, marker=\"o\", edgecolors=\"black\") #adding in the FPR, FNR rates by race\n    plt.text(row[\"FPR\"], row[\"FNR\"], f'{row[\"race\"]}', fontsize=9, ha='right', color=race_color) # labeling points\n\n# Display the plot\nplt.show()\n\nposx and posy should be finite values\nposx and posy should be finite values\n\n\n\n\n\n\n\n\n\nIf we desired to tune our classifier threshold so that the false positive rates were equal between groups, how much would we need to change the false negative rate? If we adjust the classifier threshold to equalize false positive (FP) rates across all groups, we would likely see false negative (FN) rates shift unevenly between groups.From the plot, groups with initially lower FP rates (such as Asian or American Indigenous) would require a lower threshold, increasing their FP rate while also increasing their FN rate. Conversely, groups with higher FP rates (such as Some Other Race or Two or More Races) would need a higher threshold, reducing their FP rate but also lowering their FN rate. In general, equalizing FP rates would widen the gap in FN rates, potentially exacerbating disparities in under-recognition for some groups."
  },
  {
    "objectID": "posts/blog-3/blog_3.html#discussion",
    "href": "posts/blog-3/blog_3.html#discussion",
    "title": "Blog 3- Auditing Bias",
    "section": "Discussion",
    "text": "Discussion\nEmployers looking to hire people for jobs where they want little turnover would like to use this model, it would help them understand the qualities that make a perspecitve employee retain employment for longer. Large companies who hire a lot of people at once would also benefit from using algorithimic descision making to hire people. So large companies like manufacturing or mass retaliers could benefit from a speedup in their hiring processes. From my bias audit, the impact of deploying my model could be prepetuations of racism in society, as Black people and bi-racial people are less expected to be employed than white and Asian people. Deploying this model could lead to discriminitory practices implemented in the workplace through hiring, unless it is corrected by mediating the bias in the model through hiring caps. I feel that there is strong error rate bias which disproportionatley effects bi-racial and Black people, as they are more likely to be predicted as unemployed when they are employed than employed when they are not. The other thing that would make me nervous about deploying this model is that a lot of these attributes in the dataframe are out of a person’s control, and are other forms of demographic information. Using a model like this to help in the hiring process may speed up hiring processes and eliminate some discrimination through removing a singluar judge (one hiring manager could have implicit biases), but it also may remove some benfits of merrit based hiring. People who are smart and well-suited for a job may not get a job because of demographic factors outside of their control."
  },
  {
    "objectID": "posts/blog-2/blog-2.html",
    "href": "posts/blog-2/blog-2.html",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This blog post analyzes fairness in loan approval processes. I first make a model given the training data and use the coefficents to create a score. I compare that score to a threshold to deterimine if the bank should or should not make the loan based on the training attributes. I find an average bank profit of 1820 after testing out different training features. Despite this being the optimal value, we find that the bank is likely to discriminate against people requesting medical loans, and people with lower incomes. Although these were not explicit features in the model, the model still learned that loaning to the above groups generated more risk that the bank wasn’t willing to take on."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#abstract",
    "href": "posts/blog-2/blog-2.html#abstract",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This blog post analyzes fairness in loan approval processes. I first make a model given the training data and use the coefficents to create a score. I compare that score to a threshold to deterimine if the bank should or should not make the loan based on the training attributes. I find an average bank profit of 1820 after testing out different training features. Despite this being the optimal value, we find that the bank is likely to discriminate against people requesting medical loans, and people with lower incomes. Although these were not explicit features in the model, the model still learned that loaning to the above groups generated more risk that the bank wasn’t willing to take on."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#setup",
    "href": "posts/blog-2/blog-2.html#setup",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train[\"loan_int_rate\"]= (df_train[\"loan_int_rate\"])/100 #making the loan rate into a percentage\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n0.0991\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nIn this chunk, I set up the data, reset the interest rate so it’s a percentage out of 100 and view it."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#explore",
    "href": "posts/blog-2/blog-2.html#explore",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Explore",
    "text": "Explore\n\n# data display setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n\ndata = df_train\n\n#Creating plot\nsns.displot(data=data, x=\"person_age\", hue=\"loan_intent\", multiple=\"stack\")\n\n# Adjusting x-axis limits\nplt.xlim(20, 60)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the relationship between a person’s age and their loan intent. We can see that the most loans are requested by people in their mid-20s, and are mostly for education, with medical aand venture also being top categories. As people get older, they are less likely to want a loan, but the education intent goes down and personal, home improvement and venture loan intents increase.\n\nsns.scatterplot(data= data, x= \"person_income\", y= \"loan_int_rate\", hue = \"loan_status\")\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the people who get approved for loans often have the highest interest rates, which makes sense from the bank’s perspective. A higher interest rate means the bank is making more money, which offsets the risk they take on by offering the loan. On the other hand, It’s suprising that people with higher incomes are often not approved for loans. I would expect people with higher income to be more likely to be approved for a loan, given that they have more proof they can pay. A variable that might effect this outcome is the amount they are asking for. It’s possible that people with high incomes are asking for large loans that the bank doesn’t feel comfortable taking on."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#model",
    "href": "posts/blog-2/blog-2.html#model",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Model",
    "text": "Model\n\n#data cleaning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n#cleaning data\ndf_train= df_train.dropna()\n# econding possible outcome variable\nle = LabelEncoder()\ndf_train[\"loan_intent\"]= le.fit_transform(df_train[\"loan_intent\"]) \n\n\ndf_train.head()\n\n#df_train[\"income_for_age\"]= df_train[\"person_emp_length\"]+ df_train[\"cb_person_cred_hist_length\"]\ncols= [\"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\"]\nX_train = df_train[cols]\nX_train = StandardScaler().fit_transform(X_train)  # Standardize features\ny_train = df_train[\"loan_status\"]\n\nI clean the data by dropping NA values and then create a train dataset with target columns, standardize it and then come up with y values.\n\n#modeling\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nweights= model.coef_.flatten()\nprint(weights)\n\n[-0.59390244  1.26796607  1.03233148]\n\n\nI chose to use logistic regression for this model because the goal is to predict the probability of loan default, which is a binary classification problem. Logistic regression is well-suited for this task as it provides interpretable probability outputs, allowing for clear threshold-based decision-making.\n\ndef linear_score(X, w):\n  \"\"\"\n  Calculates the linear score between features and weights\n  \"\"\"\n  return X@w # or np.dot(X, w)\n\ndf_train[\"score\"]= linear_score(X_train, weights)\n\ndf_train[\"score\"].head()\nprint(df_train[\"score\"].max())\nprint(df_train[\"score\"].min())\nprint(df_train[\"score\"].mean())\n\n8.291911080319613\n-4.45964619067289\n3.300377486570684e-16\n\n\nHere I calculate the linear score between the features (in the columns of X_train) and the weights calculated from the model."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#find-a-threshold",
    "href": "posts/blog-2/blog-2.html#find-a-threshold",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Find a Threshold",
    "text": "Find a Threshold\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n                      \nimport numpy as np\ndef profit(data, t):\n    \"\"\"\n    Calculates the profit the bank makes on a given loan\n\n    Args: \n    data: the dataframe containting the loan requesters\n    t: the threshold for accepting a loan\n\n    returns: the updated dataframe with a profit column\n    \"\"\"\n\n    #create binary yes/no values from scores based on t\n    y_pred = (data[\"score\"] &gt; t).astype(int)\n    data[\"y_pred\"]= y_pred\n    data[\"bank_profit\"] = 0.0\n    #print(y_pred, y_scores)\n    i00 = (data[\"loan_status\"] == 0)& (y_pred == 0)\n    i01 = (data[\"loan_status\"] == 0) & (y_pred == 1)\n    i10 = (data[\"loan_status\"] == 1) & (y_pred == 0)\n    i11 = (data[\"loan_status\"] == 1) &(y_pred == 1)\n    #sum up the bank profit based on the predicted values of weather or not someone will default\n    data[\"bank_profit\"][i00] = data[\"loan_amnt\"][i00] * (1 + 0.25 * data[\"loan_int_rate\"][i00]) ** 10 - data[\"loan_amnt\"][i00] # best case\n    data[\"bank_profit\"][i01] = np.nan\n    data[\"bank_profit\"][i10] = data[\"loan_amnt\"][i10] * (1 + 0.25 * data[\"loan_int_rate\"][i10]) ** 3 -1.7 * data[\"loan_amnt\"][i10] # worst case\n    data[\"bank_profit\"][i11] = np.nan\n\n    return data\n\n\nthresholds = np.linspace(-.5, 2, 101)  # Test values between -.5 and 2\nbank_profits = [profit(df_train, t)[\"bank_profit\"].mean() for t in thresholds] # calculates the average profit per borrower for each t\nprint(sum(bank_profits))\n\n\nt_star = thresholds[np.argmax(bank_profits)] #finds the argmax of the bank profits in the bank profit list\n\nprint(t_star)\n\nprofit(df_train, t_star).head()\n\n\n168930.56163829993\n0.050000000000000044\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nscore\ny_pred\nbank_profit\n\n\n\n\n1\n27\n98000\nRENT\n3.0\n1\nC\n11750\n0.1347\n0\n0.12\nY\n6\n-0.008090\n0\n4613.567568\n\n\n2\n22\n36996\nRENT\n5.0\n1\nA\n10000\n0.0751\n0\n0.27\nN\n4\n0.049639\n0\n2044.334031\n\n\n3\n24\n26000\nRENT\n2.0\n3\nC\n1325\n0.1287\n1\n0.05\nN\n4\n-0.055660\n0\n-795.445199\n\n\n4\n29\n53004\nMORTGAGE\n2.0\n2\nA\n15000\n0.0963\n0\n0.28\nN\n10\n0.375078\n1\nNaN\n\n\n6\n21\n21700\nRENT\n2.0\n2\nD\n5500\n0.1491\n1\n0.25\nN\n2\n2.595504\n1\nNaN\n\n\n\n\n\n\n\nHere I generate a profit function to calculate the best value that the bank should accept loans at as a threshold value. I do this by creating index markers (i00, i01, i10 and i11) to denote a persons loan status (default or not) and their y_pred, weather or not the bank gives them a loan. I then calculate the profitability using these markers and the equations in class. If a person never recieved a loan, the bank never recieves anything, hence the NaN. I chose to use NaN instead of 0 because it will be easier to calculate the average profit per borrower only including the people who borrowed money.\nI then run the profit function for a range of values using np.linspace to find the optimal value of t where average profit per borrower is the highest.\n\n#plotting the graph\n\nthresholds = np.linspace(-.5, 2, 101)\nprofits = [profit(df_train, t)[\"bank_profit\"].mean() for t in thresholds]\n\n# Find the best threshold\nbest_threshold = thresholds[np.argmax(profits)]\nbest_profit = max(profits)\n\n# Plot using Seaborn\nsns.lineplot(x=thresholds, y=profits)\n\n# Add vertical line for the best threshold\nplt.axvline(best_threshold, linestyle=\"--\", color=\"gray\")\n\n# Labels and title\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Expected Profit per Borrower\")\nplt.title(f\"Max Profit: {best_profit:.3f} at Threshold {best_threshold:.3f}\")\n\nplt.show()\n\n\n\n\n\n\n\n\nHere we see that the optimal value for t= 0.05, with max profit of $1820 per borrower. We can see a steady climb up to the threshold with increasing values of t and then a sharper droppoff as the bank takes on large sums of risk. An addition to the model would be evaluating the opportunity cost of rejecting a loan with someone who would pay it back to better reflect the downside to being risk-adverse"
  },
  {
    "objectID": "posts/blog-2/blog-2.html#evaluate-from-the-banks-perspective",
    "href": "posts/blog-2/blog-2.html#evaluate-from-the-banks-perspective",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Evaluate from the bank’s perspective",
    "text": "Evaluate from the bank’s perspective\n\n# Setup for test/clean the testing data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test[\"loan_int_rate\"]= (df_test[\"loan_int_rate\"])/100\ndf_test= df_test.dropna()\n\n\nX_test= df_test[cols]\nX_test = StandardScaler().fit_transform(X_test)\ny_test= df_test[\"loan_status\"]\ndf_test[\"score\"]= linear_score(X_test, weights)\nprint(t_star)\nprint(profit(df_test, t_star)[\"bank_profit\"].mean().astype(int))\ndf_test[\"score\"].head()\n\n0.050000000000000044\n1743\n\n\n0    0.486485\n1    0.999932\n2   -0.185437\n3    0.520230\n4    0.731585\nName: score, dtype: float64\n\n\nI create X and Y test here and run my profit function on it to see what the testing data looks like.\nI’m getting a similar average bank profit with the testing data, which makes sense, but at $1615, it is 200 dollars lower than that of the training data. Maybe the characteristics in the training data I used aren’t as apparent in the testing data, and maybe different patterns or stronger. It could also be that the testing data has less opporunity for bank income in the first place."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#evaluate-from-the-borrowers-perspective",
    "href": "posts/blog-2/blog-2.html#evaluate-from-the-borrowers-perspective",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Evaluate from the borrower’s perspective",
    "text": "Evaluate from the borrower’s perspective\n\nage_groups = [18, 25, 35, 50, 65, 100]\ndf_test[\"age_group\"] = pd.cut(df_test[\"person_age\"], bins=age_groups, labels=[\"18-25\", \"26-35\", \"36-50\", \"51-65\", \"65+\"])\ndf_test.groupby(\"age_group\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nage_group\ny_pred\nloan_status\n\n\n\n\n0\n18-25\n0.490789\n0.231024\n\n\n1\n26-35\n0.445140\n0.216938\n\n\n2\n36-50\n0.440767\n0.202091\n\n\n3\n51-65\n0.487805\n0.317073\n\n\n4\n65+\n0.600000\n0.200000\n\n\n\n\n\n\n\nYounger people are less likley to recieve loans, and they are also the second most likely group to default on their loans. Surpisingly, people aged 51-65 are more likely to recieve loans, but also the most likely to default on them. Given their age, I would assume that they get the benefit of the doubt and are given loans more frequently.\n\ndf_test.groupby(\"loan_intent\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nloan_intent\ny_pred\nloan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.461283\n0.287611\n\n\n1\nEDUCATION\n0.476190\n0.167517\n\n\n2\nHOMEIMPROVEMENT\n0.435065\n0.250000\n\n\n3\nMEDICAL\n0.486486\n0.284250\n\n\n4\nPERSONAL\n0.470942\n0.220441\n\n\n5\nVENTURE\n0.454357\n0.146266\n\n\n\n\n\n\n\nMedical loans aren’t super likely to given, probably because they have a higher default rate of 28%. Venture and education are most likely to be given, and both have low default rates. This is probably because both loans are seen as an investment into future great economic opportunity.\n\nincome_bins = [0, 30000, 60000, 100000, np.inf]\nincome_labels = [\"&lt;30K\", \"30K-60K\", \"60K-100K\", \"100K+\"]\ndf_test[\"income_group\"] = pd.cut(df_test[\"person_income\"], bins=income_bins, labels=income_labels)\n\ndf_test.groupby(\"income_group\")[[\"y_pred\", \"loan_status\"]].mean().reset_index()\n\n\n\n\n\n\n\n\nincome_group\ny_pred\nloan_status\n\n\n\n\n0\n&lt;30K\n0.771242\n0.460131\n\n\n1\n30K-60K\n0.557899\n0.242340\n\n\n2\n60K-100K\n0.342538\n0.136898\n\n\n3\n100K+\n0.133156\n0.110519\n\n\n\n\n\n\n\nAs income increases, people are more likely to be approved for loans and less likely to default. This makes sense, as people with greater income have more stability, and are more likely to have the capital to pay off a loan, even if their purpose in getting it didn’t pay off."
  },
  {
    "objectID": "posts/blog-2/blog-2.html#discussion",
    "href": "posts/blog-2/blog-2.html#discussion",
    "title": "Blog Two - Design and Impact of Automated Decision Systems",
    "section": "Discussion",
    "text": "Discussion\nFairness means that people are given just treatment without discrimination or bias. Therefore, this blog post analyzes questions of fairness by looking at what makes someone elligible for a loan. One of the results was that people requesting medical loans are more likely to default, thereby reducing their chances of recieving the loan. I think that this is unfair, but not from the bank’s perspecitive. As a player in a capitalist society, the bank’s responsibility is to increase profit for their shareholders. Giving out loans with higher risk of default shouldn’t be their responsibility, as it wouldn’t be fair to let the bank take the responsibility for high costs of medical care. I think that the burden of responsibility lies with the government. Healthcare shouldn’t be a for-profit industry, as profitability and health goals often do not align. Instead, what would be fair is if the government offered more healthcare benefits so people who needed medical procedures wouldn’t have to take out a loan."
  },
  {
    "objectID": "posts/blog-1/blog_1.html",
    "href": "posts/blog-1/blog_1.html",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "image.png\n\n\n\n\nIn my blog post, I explore the best features to identify penguin species from the Palmer Penguins data set created by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Dr. Kristen Gorman ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Palmer Station. I first explore the significance of some variables through data visualization, trying to glean key differences in penguin species, one of which appears to be island. Using a brute force search of all the possible combinations of one qualitative and two quantitative variables, I find that island, culmen length, and culmen depth are the three best features to predict the test set with 100% accuracy.\n\n\n\n\nimport pandas as pd\n\n# reading in the training data\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url) \n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can see from the head of the training data some of the key variables to look at: island, clutch completion, culmen length/depth, flipper length and body mass. These variables jump out as the oens where we will likely see the biggest differences between species.\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #encoding species\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #dropping uncessary variables\n  df = df[df[\"Sex\"] != \".\"] \n  df = df.dropna() #cleaning data\n  y = le.transform(df[\"Species\"]) #encoding species\n  df = df.drop([\"Species\"], axis = 1) \n  df = pd.get_dummies(df) \n  return df, y\n\n#creating test and train datasets\nX_train, y_train = prepare_data(train) \n\nIn the data preperation section, I prepare the data by dropping the variables that aren’t helpful for the analysis. I also use the label encoder function to encode the target variable (species). I also clean the data so I can prepare the features and the labels for training.\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n## graph1 scatter + weight distribtuion\n\ndata= train\ngraph1 = sns.jointplot(data= data, x= \"Flipper Length (mm)\", y= \"Body Mass (g)\", hue = \"Species\")\nplt.show\n\n\n\n\n\n\n\n\nThe main takeaway I had from this graph is that flipper length is a great identifier for the Gentoo penguins, as they all have flipper lengths at about 208 mm or above, which is much longer than the average of an Adelie or a Chinstrap penguin. However, the body mass and flipper length are similar for and Chinstrap penguins, which means that they might not be the best identifier of a penguin. We can see that the distributions for body mass of the two species have almost complete overlap, with Adelie penguins having less devation in mass by penguin.\n\n# graph 2: catplot\ndata = train\ngraph2= sns.catplot(data= data, kind= \"swarm\", x= \"Island\", y=\"Culmen Length (mm)\", hue= \"Species\")\n\n\n\n\n\n\n\n\nThis visualization is very helpful for deciding which categories to model. The first thing I notice is that the Chinstrap and Gentoo species are native to one island and one island only (dream and biscoe, respectiveley). While the Adelie penguins appear on all 3 islands, they are the only ones that appear on Torgenrsen, so any penguin found on Torgersen should automatically be identified as a Adelie. Culmen length is also a good diffentiator,\n\ndata.groupby(\"Species\").aggregate({\"Flipper Length (mm)\" : \"mean\", \n                                   \"Culmen Length (mm)\" : \"mean\",\n                                    \"Culmen Depth (mm)\" : \"mean\",\n                                    \"Body Mass (g)\" : [\"mean\", \"min\", \"max\"]})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nmean\nmean\nmean\nmin\nmax\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n3718.487395\n2850.0\n4725.0\n\n\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n3743.421053\n2700.0\n4800.0\n\n\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n5039.948454\n3950.0\n6300.0\n\n\n\n\n\n\n\nThis table shows clear distinctions between the penguins for the different quantitative variables highlighted here. For example, while Adelie and Chinstraps have similar flipper lengths, masses, and culmen depths, they vary vastly in clumen length. Gentoos can be differentiated through their much larger average body mass and longer flippers. There shouild be very little overlap betrween Adelie and Gentoo, as they very significantly on the different quantitative variables.\n\n\n\n\n%%capture\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Stage\", \"Sex\"] \nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\" ]\n\nbest_score= 0\nfor qual in all_qual_cols: #iterating through all possible combinations of columns\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2): \n    cols = list(pair) + qual_cols\n    #print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train) #fitting the model\n    score = LR.score(X_train[cols], y_train) #scoring the model\n    if score &gt; best_score: #saving the best score\n      best_score = score\n      best_cols = cols\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\nThe way I approach selecting the best features is to use a nested loop to run through all possible combinations of one qualitative and two quantitative variables. I keep track of the training model with the best score and save those features.\n\n%%capture\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train) #running the model on the best features\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nHere I run the training model on my best selected features and load in the test data.\n\n#preparing the test set and testing it\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nI calculate the score here for the model established on the testing data and get an accuracy score of 100%.\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nprint(best_cols)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# plotting the decision reigon plot\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(LR, X_train[best_cols], y_train)\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the decision plots for the training and the testing data sets. As we saw in the explore section, certain species are native to certain islands. In the trainig data, we see that some Chinstrap penguins are on the boundary between Chinstrap and Gentoo reigons (causing slight misclassification), but due to the subset of data in the testing set, none of these penguins are super close to the boarder, so everything is classified correctly.\n\nfrom sklearn.metrics import confusion_matrix\n\n# creating confusion matricies for the test and train datasets.\ny_test_pred = LR.predict(X_test[best_cols])\nConfusion_test = confusion_matrix(y_test, y_test_pred)\nprint(Confusion_test) \ny_train_pred = LR.predict(X_train[best_cols])\nConfusion_train = confusion_matrix(y_train, y_train_pred)\nprint(Confusion_train)\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n[[108   0   0]\n [  1  55   0]\n [  0   0  92]]\n\n\nSince there are no errors in the test set, the first confusion matrix is diagonalizable. There are no penguins that are classified incorrectly. Because of this, I also created the same matrix for the train dataset, where one Chinstrap penguin was classified an Adelie. I wonder if this could be in part because the number of penguins in each set where drastically different, and since there a lot more Adelie I wonder if penguins were more likely to be classified as such.\n\n\n\nMy key takeaway from the analysis is that when using one feature on it’s own, or a pair of correlating features only (i.e. culmen length and culmen depth), logistic regression is decently accurate. The more features you pair together, the more accurate the logistic regression becomes, although the computing power escalates quickly. Island is a great quantitative identifier for penguins, as shown in figure 2 because Chinstraps and Gentoos exist only on different islands, so there is no overlap or mispredicting.\nOn competing my first blog post, I found the process very rewarding. I like being able to write out my ideas and complete my analysis all in the same document. I haven’t coded in a while, so debugging at first was difficult, but it got easier as I went on. I learned that, with both the coding and the discussion, it’s often best to step away for a bit and come back to the work with fresh eyes. That helped me when I was looking for significance in the explore step, and throughout debugging my model and the ensuing discussion."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#abstract",
    "href": "posts/blog-1/blog_1.html#abstract",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "In my blog post, I explore the best features to identify penguin species from the Palmer Penguins data set created by ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Dr. Kristen Gorman ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍Palmer Station. I first explore the significance of some variables through data visualization, trying to glean key differences in penguin species, one of which appears to be island. Using a brute force search of all the possible combinations of one qualitative and two quantitative variables, I find that island, culmen length, and culmen depth are the three best features to predict the test set with 100% accuracy."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#setup",
    "href": "posts/blog-1/blog_1.html#setup",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "import pandas as pd\n\n# reading in the training data\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url) \n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nWe can see from the head of the training data some of the key variables to look at: island, clutch completion, culmen length/depth, flipper length and body mass. These variables jump out as the oens where we will likely see the biggest differences between species."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#data-preperation",
    "href": "posts/blog-1/blog_1.html#data-preperation",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #encoding species\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #dropping uncessary variables\n  df = df[df[\"Sex\"] != \".\"] \n  df = df.dropna() #cleaning data\n  y = le.transform(df[\"Species\"]) #encoding species\n  df = df.drop([\"Species\"], axis = 1) \n  df = pd.get_dummies(df) \n  return df, y\n\n#creating test and train datasets\nX_train, y_train = prepare_data(train) \n\nIn the data preperation section, I prepare the data by dropping the variables that aren’t helpful for the analysis. I also use the label encoder function to encode the target variable (species). I also clean the data so I can prepare the features and the labels for training."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#explore",
    "href": "posts/blog-1/blog_1.html#explore",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\n## graph1 scatter + weight distribtuion\n\ndata= train\ngraph1 = sns.jointplot(data= data, x= \"Flipper Length (mm)\", y= \"Body Mass (g)\", hue = \"Species\")\nplt.show\n\n\n\n\n\n\n\n\nThe main takeaway I had from this graph is that flipper length is a great identifier for the Gentoo penguins, as they all have flipper lengths at about 208 mm or above, which is much longer than the average of an Adelie or a Chinstrap penguin. However, the body mass and flipper length are similar for and Chinstrap penguins, which means that they might not be the best identifier of a penguin. We can see that the distributions for body mass of the two species have almost complete overlap, with Adelie penguins having less devation in mass by penguin.\n\n# graph 2: catplot\ndata = train\ngraph2= sns.catplot(data= data, kind= \"swarm\", x= \"Island\", y=\"Culmen Length (mm)\", hue= \"Species\")\n\n\n\n\n\n\n\n\nThis visualization is very helpful for deciding which categories to model. The first thing I notice is that the Chinstrap and Gentoo species are native to one island and one island only (dream and biscoe, respectiveley). While the Adelie penguins appear on all 3 islands, they are the only ones that appear on Torgenrsen, so any penguin found on Torgersen should automatically be identified as a Adelie. Culmen length is also a good diffentiator,\n\ndata.groupby(\"Species\").aggregate({\"Flipper Length (mm)\" : \"mean\", \n                                   \"Culmen Length (mm)\" : \"mean\",\n                                    \"Culmen Depth (mm)\" : \"mean\",\n                                    \"Body Mass (g)\" : [\"mean\", \"min\", \"max\"]})\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\nBody Mass (g)\n\n\n\nmean\nmean\nmean\nmean\nmin\nmax\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n3718.487395\n2850.0\n4725.0\n\n\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n3743.421053\n2700.0\n4800.0\n\n\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n5039.948454\n3950.0\n6300.0\n\n\n\n\n\n\n\nThis table shows clear distinctions between the penguins for the different quantitative variables highlighted here. For example, while Adelie and Chinstraps have similar flipper lengths, masses, and culmen depths, they vary vastly in clumen length. Gentoos can be differentiated through their much larger average body mass and longer flippers. There shouild be very little overlap betrween Adelie and Gentoo, as they very significantly on the different quantitative variables."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#model",
    "href": "posts/blog-1/blog_1.html#model",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "%%capture\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\", \"Stage\", \"Sex\"] \nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\" ]\n\nbest_score= 0\nfor qual in all_qual_cols: #iterating through all possible combinations of columns\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2): \n    cols = list(pair) + qual_cols\n    #print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train) #fitting the model\n    score = LR.score(X_train[cols], y_train) #scoring the model\n    if score &gt; best_score: #saving the best score\n      best_score = score\n      best_cols = cols\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\nThe way I approach selecting the best features is to use a nested loop to run through all possible combinations of one qualitative and two quantitative variables. I keep track of the training model with the best score and save those features.\n\n%%capture\nLR = LogisticRegression()\nLR.fit(X_train[best_cols], y_train) #running the model on the best features\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nHere I run the training model on my best selected features and load in the test data.\n\n#preparing the test set and testing it\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nI calculate the score here for the model established on the testing data and get an accuracy score of 100%."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#evaluate",
    "href": "posts/blog-1/blog_1.html#evaluate",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nprint(best_cols)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# plotting the decision reigon plot\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(LR, X_train[best_cols], y_train)\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the decision plots for the training and the testing data sets. As we saw in the explore section, certain species are native to certain islands. In the trainig data, we see that some Chinstrap penguins are on the boundary between Chinstrap and Gentoo reigons (causing slight misclassification), but due to the subset of data in the testing set, none of these penguins are super close to the boarder, so everything is classified correctly.\n\nfrom sklearn.metrics import confusion_matrix\n\n# creating confusion matricies for the test and train datasets.\ny_test_pred = LR.predict(X_test[best_cols])\nConfusion_test = confusion_matrix(y_test, y_test_pred)\nprint(Confusion_test) \ny_train_pred = LR.predict(X_train[best_cols])\nConfusion_train = confusion_matrix(y_train, y_train_pred)\nprint(Confusion_train)\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n[[108   0   0]\n [  1  55   0]\n [  0   0  92]]\n\n\nSince there are no errors in the test set, the first confusion matrix is diagonalizable. There are no penguins that are classified incorrectly. Because of this, I also created the same matrix for the train dataset, where one Chinstrap penguin was classified an Adelie. I wonder if this could be in part because the number of penguins in each set where drastically different, and since there a lot more Adelie I wonder if penguins were more likely to be classified as such."
  },
  {
    "objectID": "posts/blog-1/blog_1.html#discussion",
    "href": "posts/blog-1/blog_1.html#discussion",
    "title": "Blog One - Palmer Penguins",
    "section": "",
    "text": "My key takeaway from the analysis is that when using one feature on it’s own, or a pair of correlating features only (i.e. culmen length and culmen depth), logistic regression is decently accurate. The more features you pair together, the more accurate the logistic regression becomes, although the computing power escalates quickly. Island is a great quantitative identifier for penguins, as shown in figure 2 because Chinstraps and Gentoos exist only on different islands, so there is no overlap or mispredicting.\nOn competing my first blog post, I found the process very rewarding. I like being able to write out my ideas and complete my analysis all in the same document. I haven’t coded in a while, so debugging at first was difficult, but it got easier as I went on. I learned that, with both the coding and the discussion, it’s often best to step away for a bit and come back to the work with fresh eyes. That helped me when I was looking for significance in the explore step, and throughout debugging my model and the ensuing discussion."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 3- Auditing Bias\n\n\n\n\n\nAuditing Employment Bias\n\n\n\n\n\nMar 4, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Two - Design and Impact of Automated Decision Systems\n\n\n\n\n\nBank Loaning\n\n\n\n\n\nFeb 27, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nBlog One - Palmer Penguins\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nFeb 16, 2025\n\n\nChloe Katz\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]